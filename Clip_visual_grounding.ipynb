{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcXWWy-L8pC6"
   },
   "source": [
    "# Environment setup ============================================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXgkDdgMy00s"
   },
   "source": [
    "## Installing CLIP and YoloV5 and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UV4kP__zzi4v"
   },
   "source": [
    "It the first section of this file, the installation of the needed components is performed. These first bash lines install CLIP and YoloV5 respectively. These two Neural Network will represent the ground base of the project development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZafaZHs7O3Fn",
    "outputId": "0af4abe0-dabc-4d57-b72d-673f6d2ef4fd"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "# Download CLIP and YOLO\n",
    "pip install git+https://github.com/openai/CLIP.git\n",
    "pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt\n",
    "\n",
    "# Command to install some needed dependencies in the AWS machine\n",
    "sudo apt-get update && sudo apt-get install ffmpeg libsm6 libxext6 -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVRByk6a0txX"
   },
   "source": [
    "## List of imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "24c7NKAZUpl3"
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "import pickle\n",
    "import json\n",
    "import tarfile\n",
    "import os\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# utility libraries imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# torch imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO26z6XD6K4b"
   },
   "source": [
    "## Setting the Clip model and Yolo model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dusz3KE86Jka",
    "outputId": "4b846792-9e80-4eeb-ba82-888648cc72e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sagemaker-user/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2024-11-17 Python-3.11.10 torch-2.3.1.post300 CUDA:0 (Tesla T4, 14918MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# Chosing the device \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# choosing the clip model and the yolo versions\n",
    "clip_model, preprocess = clip.load('RN50', device)\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDi1EewJ6oNM"
   },
   "source": [
    "# Fine-tuning Clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1dkP5vnlaAu"
   },
   "source": [
    "## Creation of the train and validation splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GYXvuG3kW2c"
   },
   "source": [
    "The following code sections contain the needed structures to load the data from the refcoco dataset.\n",
    "The structures needed are:\n",
    "1. a Dataset Class to load the Data\n",
    "2. a Dataloader instantiation, to be used to split data into batches and that will be used to iterate throiugh for train, validation and test\n",
    "\n",
    "The purpose of the Refcocog is Referring Expression Grounding, whose goal is to identify an object given a referring example. This is corresponds with the objective of this project.\n",
    "\n",
    "The dataset is composed of 25799 images, each having an average of 3.7 referring expression. These expression are related to specific objects inside the image. The Ground truth is represented by the bounding boxes.\n",
    "\n",
    "The set of file composing the dataset are:\n",
    " - instances.json which contains all the information about the bunding boxes of each image\n",
    "   example of instance\n",
    " - ref(umd).p which is a serialized file with all the description related to a bounding box and the split it belongs to (train/validation/test)\n",
    " - the images directory with all the images\n",
    "\n",
    "This Dataset class, reads the instances.json and refs(umd).p files, creates an association image_id->image_name and annotation_id -> bounding_boxes to simplify the retrivial of the single element in the getitem() method.\n",
    "Moreover, a set of samples is created with all the datase entries, each seample is composed of: image id, annotation id, and the sentence. The oobjective of this structure, besides contaioning all samples for the len() method, is to simplify the implementation of the getitem method.\n",
    "The latter takes as input an idx (which is the element currently being processed by the iterator) and return the image cropped to the bounding boxes and the sentence related with that box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefCOCOgDataset(Dataset):\n",
    "    def __init__(self, transform=None, split='train', device='cuda', crop=False):\n",
    "        # Load images and transform\n",
    "        self.image_dir = os.path.join('refcocog', 'images')\n",
    "        self.transform = transform\n",
    "\n",
    "        # Define class properties for split and device\n",
    "        self.split = split\n",
    "        self.device = device\n",
    "        self.crop = crop\n",
    "\n",
    "        # Load data from ref(umd) and instances files\n",
    "        self.refs = self.load_refs()\n",
    "        self.instances = self.load_instances()\n",
    "\n",
    "        # Create efficient lookup dictionaries\n",
    "        self.image_id_to_filename = {img['id']: img['file_name'] \n",
    "                                   for img in self.instances['images']}\n",
    "        self.ann_id_to_bbox = {ann['id']: ann['bbox'] \n",
    "                              for ann in self.instances['annotations']}\n",
    "\n",
    "        # Prepare samples\n",
    "        self.samples = self._prepare_samples()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        # Load and process image\n",
    "        image_name = self.image_id_to_filename[sample['image_id']]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Get and process bbox\n",
    "        bbox = self.ann_id_to_bbox[sample['ann_id']]\n",
    "\n",
    "        # Get and process bbox\n",
    "        box = self.ann_id_to_bbox[sample['ann_id']]\n",
    "        x1, y1, w, h = box\n",
    "        x2, y2 = x1 + w, y1 + h\n",
    "\n",
    "        # Optional: crop image to bounding boxes for fine-tuning\n",
    "        if self.crop:\n",
    "            # Ensure bbox coordinates are valid\n",
    "            x1 = max(0, int(x1))\n",
    "            y1 = max(0, int(y1))\n",
    "            x2 = min(image.size[0], int(x2))\n",
    "            y2 = min(image.size[1], int(y2))\n",
    "\n",
    "            # Crop and transform\n",
    "            image = image.crop((x1, y1, x2, y2))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Sample to return\n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'sentence': sample['sentence'],\n",
    "            'bbox': bbox\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "    def load_refs(self):\n",
    "        annotation_file = os.path.join('refcocog', 'annotations', 'refs(umd).p')\n",
    "\n",
    "        with open(annotation_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return [item for item in data if item['split'] == self.split]\n",
    "\n",
    "    def load_instances(self):\n",
    "        instances_file = os.path.join('refcocog', 'annotations', 'instances.json')\n",
    "        with open(instances_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        samples = []\n",
    "        for ref in self.refs:\n",
    "            for sentence in ref['sentences']:\n",
    "                samples.append({\n",
    "                    'image_id': ref['image_id'],\n",
    "                    'ann_id': ref['ann_id'],\n",
    "                    'sentence': sentence['sent']\n",
    "                })\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different dasets class are instantiated, one for the train set, one for the validation test, and one for the test set.\n",
    "\n",
    "Each class is then loaded in a DataLoader wrapper. All these dataloader have been designed to work leveraging multithreading, with the goal of speeding up training and validation.\n",
    "It is important to point out that while the train set is shuffled, the validation and test set are not, since it wouyld be pointless to shuffle them. \n",
    "Moreover, data are split in batches whose size is 64. This parameter has also been chosen for speed reason, and 64 elements batches represend a good tradeof, since batches are nor too large or too somal, and the update of the weights happens after a reasonable amount of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nuQnqiR5ykkO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Number of training samples: 80512\n",
      "Number of validation samples: 4896\n",
      "Number of test samples: 9602\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "# Train, test, and validation set split cropping images\n",
    "finetune_train_dataset = RefCOCOgDataset(transform=preprocess, split='train', crop=True)\n",
    "finetune_val_dataset   = RefCOCOgDataset(transform=preprocess, split='val', crop=True)\n",
    "finetune_test_dataset  = RefCOCOgDataset(transform=preprocess, split='test', crop=True)\n",
    "\n",
    "# DataLoaders batch size and other options. Computation is done with 4 workers to speed it up\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "pin_memory = True\n",
    "persistent_workers = True\n",
    "\n",
    "# DataLoader, to create iterable batches with 32 examples each, shuffled in case of training set and not shuffled in case of validation set\n",
    "finetune_train_loader = DataLoader(\n",
    "    dataset=finetune_train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "finetune_val_loader = DataLoader(\n",
    "    dataset=finetune_val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "finetune_test_loader = DataLoader(\n",
    "    dataset=finetune_test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "print(\"=======================================================\")\n",
    "print(\"Number of training samples:\",len(finetune_train_dataset))\n",
    "print(\"Number of validation samples:\",len(finetune_val_dataset))\n",
    "print(\"Number of test samples:\",len(finetune_test_dataset))\n",
    "print(\"=======================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8imolEUuzAkt"
   },
   "source": [
    "## Training and storing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhAZhmvoqi44"
   },
   "source": [
    "**Train** and **Validation** functions for each training epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oK-G1UE2ttsq"
   },
   "outputs": [],
   "source": [
    "# Learning rate and optimizer\n",
    "# learning_rate = 1e-3\n",
    "# optimizer = Adam(clip_model.parameters(), lr=learning_rate)\n",
    "optimizer = Adam(clip_model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "\n",
    "# Loss functions\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, dataloader, optimizer, device, transform=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, total=len(dataloader))\n",
    "\n",
    "    for batch in pbar:\n",
    "        images = batch[\"image\"].to(device, non_blocking=True)\n",
    "        texts = clip.tokenize(batch[\"sentence\"]).to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "        # Compute loss\n",
    "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "        loss_image = loss_img(logits_per_image, ground_truth)\n",
    "        loss_text = loss_txt(logits_per_text, ground_truth)\n",
    "        loss = (loss_image + loss_text) / 2\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_description(f'Loss: {loss.item():.4f}')\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, dataloader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    pbar = tqdm(dataloader, total=len(dataloader), desc='Validation')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            # Correctly extract images and texts from the batch\n",
    "            images = batch[0].to(device)\n",
    "            texts = clip.tokenize(batch[1]).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "            # Calculate loss\n",
    "            ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "            loss_image = loss_img(logits_per_image, ground_truth)\n",
    "            loss_text = loss_txt(logits_per_text, ground_truth)\n",
    "            loss = (loss_image + loss_text) / 2\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(logits_per_image, dim=1)\n",
    "            accuracy = (predictions == ground_truth).float().mean()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += accuracy.item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_description(f'Val Loss: {loss.item():.4f} | Val Accuracy: {accuracy.item():.4f}')\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_accuracy = total_accuracy / len(dataloader)\n",
    "\n",
    "    print(f\"\\nValidation Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "am1sLw6urCfu"
   },
   "source": [
    "Training loop that generates the pretrained clip model on refCocog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-9gTgwgq9ol",
    "outputId": "8d7d9af4-21b8-4b8b-dd08-13649b502a60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.4296:  21%|â–ˆâ–ˆ        | 266/1258 [04:38<17:15,  1.04s/it]"
     ]
    }
   ],
   "source": [
    "# Ensure the model is in float32 precision and transferred to the correct device\n",
    "clip_model = clip_model.to(device).float()\n",
    "\n",
    "# Number of epochs for training\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, num_epochs + 1):  # Start epochs from 1 for readability\n",
    "    print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "\n",
    "    # Train for one epoch\n",
    "    train_loss = train_epoch(clip_model, finetune_train_loader, optimizer, device)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validate after each epoch\n",
    "    val_loss, val_accuracy = validate(clip_model, finetune_val_loader, device)\n",
    "    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Save the final fine-tuned model\n",
    "torch.save(clip_model.state_dict(), 'fine_tuned_clip_refcocog_final.pth')\n",
    "print(\"\\nTraining complete. Model saved as 'fine_tuned_clip_refcocog_final.pth'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZiwascG1hrj"
   },
   "source": [
    "# First Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoyviWj_83pG",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## New Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accomplish the goal of this project, a new Datasset class is defined. This functions the same as RefCOCOgDatasetFineTuning, but the getItem method returns a vector of sentence and a bounding box representing the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefCOCOgDatasetFineTuning(Dataset):\n",
    "    def __init__(self, transform=None, split='train', device='cuda'):\n",
    "        # Load images and transform \n",
    "        self.image_dir = os.path.join('refcocog', 'images')\n",
    "        self.transform = transform # or transforms.Compose([\n",
    "        #     transforms.Resize((224, 224)),\n",
    "        #     transforms.ToTensor(),\n",
    "        #     transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "        #                        (0.26862954, 0.26130258, 0.27577711))\n",
    "        # ])\n",
    "\n",
    "        # Define class properties for split and device\n",
    "        self.split = split\n",
    "        self.device = device\n",
    "\n",
    "        # Load data from ref(umd) and instances files\n",
    "        self.refs = self.load_refs()\n",
    "        self.instances = self.load_instances()\n",
    "\n",
    "        # Create efficient lookup dictionaries\n",
    "        self.image_id_to_filename = {img['id']: img['file_name'] \n",
    "                                   for img in self.instances['images']}\n",
    "        self.ann_id_to_bbox = {ann['id']: ann['bbox'] \n",
    "                              for ann in self.instances['annotations']}\n",
    "\n",
    "        # Prepare samples\n",
    "        self.samples = self._prepare_samples()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        # Load and process image\n",
    "        image_name = self.image_id_to_filename[sample['image_id']]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Get and process bbox\n",
    "        box = self.ann_id_to_bbox[sample['ann_id']]\n",
    "        x1, y1, w, h = box\n",
    "        x2, y2 = x1 + w, y1 + h\n",
    "\n",
    "        # Ensure bbox coordinates are valid\n",
    "        x1 = max(0, int(x1))\n",
    "        y1 = max(0, int(y1))\n",
    "        x2 = min(image.size[0], int(x2))\n",
    "        y2 = min(image.size[1], int(y2))\n",
    "\n",
    "        # Crop and transform\n",
    "        cropped_image = image.crop((x1, y1, x2, y2))\n",
    "        if self.transform:\n",
    "            cropped_image = self.transform(cropped_image)\n",
    "\n",
    "        return cropped_image, sample['sentence']\n",
    "\n",
    "    def load_refs(self):\n",
    "        annotation_file = os.path.join('refcocog', 'annotations', 'refs(umd).p')\n",
    "\n",
    "        with open(annotation_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return [item for item in data if item['split'] == self.split]\n",
    "\n",
    "    def load_instances(self):\n",
    "        instances_file = os.path.join('refcocog', 'annotations', 'instances.json')\n",
    "        with open(instances_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        samples = []\n",
    "        for ref in self.refs:\n",
    "            for sentence in ref['sentences']:\n",
    "                samples.append({\n",
    "                    'image_id': ref['image_id'],\n",
    "                    'ann_id': ref['ann_id'],\n",
    "                    'sentence': sentence['sent']\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoyviWj_83pG"
   },
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8vsVZcZ1ovp"
   },
   "source": [
    "The base model is defined as a starting point to further study the task and become familiar with this visual grounding task. The approach is described in the project statement and is udeful to get familiar with the visual grounding task. \n",
    "The idea is to combine YOLO to compute the bounding boxes and CLIP to find the best cropped image representing the text input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYKYEEyk7kxy"
   },
   "outputs": [],
   "source": [
    "class YoloClip(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, clip_model, yolo_model):\n",
    "        super().__init__()\n",
    "        self.clip_model = clip_model # take the pretrained clip model\n",
    "        self.yolo_model = yolo_model\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        # Obtain yolo detections for the original image\n",
    "        yolo_results = self.yolo_model(image)\n",
    "\n",
    "        # Get crops from the detections\n",
    "        yolo_crop = yolo_results.crop(save = False)\n",
    "\n",
    "        # Get the results in pandas form to get the number of crops for each image\n",
    "        yolo_panda = yolo_results.pandas().xyxy\n",
    "\n",
    "        crops_nums = [len(yolo_panda[i]) for i in range(len(image))]\n",
    "\n",
    "        # Preprocess the cropped images before passing it to CLIP\n",
    "        # crop_images = torch.stack([self.image_transform(crop['im'].copy()) for crop in yolo_crop]).to(device)\n",
    "        # crop_classes = [yolo_results.names.get(crop['cls'].item()) for crop in yolo_crop]\n",
    "        # crop_classes_tensor = names_to_coco_cats(crop_classes)\n",
    "        # crop_classes_tensor = crop_classes_tensor.to(device)\n",
    "\n",
    "        # Get the CLIP embedding for each of the cropped images\n",
    "        # image_features = self.clip_model.encode_image(crop_images)\n",
    "        # image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Get the CLIP embedding for each of the text tokens\n",
    "        # text = clip.tokenize(text).to(device)\n",
    "        # text_features = self.clip_model.encode_text(text)\n",
    "        # text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFEnidQR8pwb"
   },
   "source": [
    "Instantiating and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsx6uOd98pH8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGMh259V3iho"
   },
   "source": [
    "# Second Model: XGBoost integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_tQeasK3tKG"
   },
   "source": [
    "Describe the integration with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqu_koz-zW7W"
   },
   "source": [
    "# Bash utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OKAa_50TkWTv",
    "outputId": "7d3b3efc-975a-4d84-d3c1-4f6686fa3671"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "tar -xzvf refcocog.tar.gz"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "RVRByk6a0txX",
    "WGMh259V3iho",
    "uqu_koz-zW7W"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
