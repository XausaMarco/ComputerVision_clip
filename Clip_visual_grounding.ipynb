{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcXWWy-L8pC6"
   },
   "source": [
    "# Environment setup \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXgkDdgMy00s"
   },
   "source": [
    "## Installing CLIP and YoloV5 and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UV4kP__zzi4v"
   },
   "source": [
    "It the first section of this file, the installation of the needed components is performed. These first bash lines install CLIP and YoloV5 respectively. These two Neural Network will represent the ground base of the project development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZafaZHs7O3Fn",
    "outputId": "0af4abe0-dabc-4d57-b72d-673f6d2ef4fd"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "# Download CLIP and YOLO\n",
    "pip install git+https://github.com/openai/CLIP.git\n",
    "pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt\n",
    "\n",
    "# Command to install some needed dependencies in the AWS machine\n",
    "sudo apt-get update && sudo apt-get install ffmpeg libsm6 libxext6 -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVRByk6a0txX"
   },
   "source": [
    "## List of imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24c7NKAZUpl3"
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "import pickle\n",
    "import json\n",
    "import tarfile\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# utility libraries imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# torch imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO26z6XD6K4b"
   },
   "source": [
    "## Setting the Clip model and Yolo model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dusz3KE86Jka",
    "outputId": "4b846792-9e80-4eeb-ba82-888648cc72e3"
   },
   "outputs": [],
   "source": [
    "# Chosing the device \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# choosing the clip model and the yolo versions, both pre-trained\n",
    "clip_model, preprocess = clip.load('RN50', device)\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True, trust_repo=True)\n",
    "\n",
    "# Ensure the model is in float32 precision and transferred to the correct device\n",
    "clip_model = clip_model.to(device).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDi1EewJ6oNM"
   },
   "source": [
    "# Fine-tuning Clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1dkP5vnlaAu"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GYXvuG3kW2c"
   },
   "source": [
    "The following code sections contain the needed structures to load the data from the refcoco dataset.\n",
    "The structures needed are:\n",
    "1. a Dataset Class to load the Data\n",
    "2. a Dataloader instantiation, to be used to split data into batches and that will be used to iterate throiugh for train, validation and test\n",
    "\n",
    "The purpose of the Refcocog is Referring Expression Grounding, whose goal is to identify an object given a referring example. This is corresponds with the objective of this project.\n",
    "\n",
    "The dataset is composed of 25799 images, each having an average of 3.7 referring expression. These expression are related to specific objects inside the image. The Ground truth is represented by the bounding boxes.\n",
    "\n",
    "The set of file composing the dataset are:\n",
    " - instances.json which contains all the information about the bunding boxes of each image\n",
    "   example of instance\n",
    " - ref(umd).p which is a serialized file with all the description related to a bounding box and the split it belongs to (train/validation/test)\n",
    " - the images directory with all the images\n",
    "\n",
    "This Dataset class, reads the instances.json and refs(umd).p files, creates an association image_id->image_name and annotation_id -> bounding_boxes to simplify the retrivial of the single element in the getitem() method.\\\n",
    "Moreover, a set of samples is created with all the datase entries, each seample is composed of: image id, annotation id, and the sentence. The oobjective of this structure, besides contaioning all samples for the len() method, is to simplify the implementation of the getitem method.\\\n",
    "The latter takes as input an idx (which is the element currently being processed by the iterator) and return the image cropped to the bounding boxes and the sentence related with that box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefCOCOgDataset(Dataset):\n",
    "    def __init__(self, transform=None, split='train', device='cuda', crop=False):\n",
    "        # Load images and transform\n",
    "        self.image_dir = os.path.join('refcocog', 'images')\n",
    "        self.transform = transform\n",
    "\n",
    "        # Define class properties for split and device\n",
    "        self.split = split\n",
    "        self.device = device\n",
    "        self.crop = crop\n",
    "\n",
    "        # Load data from ref(umd) and instances files\n",
    "        self.refs = self.load_refs()\n",
    "        self.instances = self.load_instances()\n",
    "\n",
    "        # Create efficient lookup dictionaries\n",
    "        self.image_id_to_filename = {img['id']: img['file_name'] \n",
    "                                   for img in self.instances['images']}\n",
    "        self.ann_id_to_bbox = {ann['id']: ann['bbox'] \n",
    "                              for ann in self.instances['annotations']}\n",
    "\n",
    "        # Prepare samples\n",
    "        self.samples = self._prepare_samples()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        # Load and process image\n",
    "        image_name = self.image_id_to_filename[sample['image_id']]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Get and process bbox\n",
    "        bbox = self.ann_id_to_bbox[sample['ann_id']]\n",
    "\n",
    "        # Get and process bbox\n",
    "        box = self.ann_id_to_bbox[sample['ann_id']]\n",
    "        x1, y1, w, h = box\n",
    "        x2, y2 = x1 + w, y1 + h\n",
    "\n",
    "        # Optional: crop image to bounding boxes (for CLIP fine-tuning)\n",
    "        if self.crop:\n",
    "            # Ensure bbox coordinates are valid\n",
    "            x1 = max(0, int(x1))\n",
    "            y1 = max(0, int(y1))\n",
    "            x2 = min(image.size[0], int(x2))\n",
    "            y2 = min(image.size[1], int(y2))\n",
    "\n",
    "            # Crop and transform\n",
    "            image = image.crop((x1, y1, x2, y2))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Sample to return\n",
    "        sample = {\n",
    "            'image_path':image,\n",
    "            'preprocessed_image': image,\n",
    "            'sentence': sample['sentence'],\n",
    "            'bbox': torch.Tensor([x1,y1,x2,y2])\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "    def load_refs(self):\n",
    "        annotation_file = os.path.join('refcocog', 'annotations', 'refs(umd).p')\n",
    "\n",
    "        with open(annotation_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return [item for item in data if item['split'] == self.split]\n",
    "\n",
    "    def load_instances(self):\n",
    "        instances_file = os.path.join('refcocog', 'annotations', 'instances.json')\n",
    "        with open(instances_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        samples = []\n",
    "        for ref in self.refs:\n",
    "            for sentence in ref['sentences']:\n",
    "                samples.append({\n",
    "                    'image_id': ref['image_id'],\n",
    "                    'ann_id': ref['ann_id'],\n",
    "                    'sentence': sentence['sent']\n",
    "                })\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different dasets class are instantiated, one for the train set, one for the validation test, and one for the test set. \n",
    "\n",
    "Each class is then loaded in a DataLoader wrapper. All these dataloader have been designed to work leveraging multithreading, with the goal of speeding up training and validation.\\\n",
    "It is important to point out that while the train set is shuffled, the validation and test set are not, since it wouyld be pointless to shuffle them. \\\n",
    "Moreover, data are split in batches whose size is 64. This parameter has also been chosen for speed reason, and 64 elements batches represent a good trade-of, since batches are nor too large or too somal, and the update of the weights happens after a reasonable amount of examples (given the dataset size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuQnqiR5ykkO"
   },
   "outputs": [],
   "source": [
    "# Train, test, and validation set split cropping images\n",
    "finetune_train_dataset = RefCOCOgDataset(transform=preprocess, split='train', crop=True)\n",
    "finetune_val_dataset = RefCOCOgDataset(transform=preprocess, split='val', crop=True)\n",
    "finetune_test_dataset = RefCOCOgDataset(transform=preprocess, split='test', crop=True)\n",
    "\n",
    "# DataLoaders batch size and other options. Computation is done with 4 workers to speed it up\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "pin_memory = True\n",
    "persistent_workers = True\n",
    "\n",
    "# DataLoader, to create iterable batches with 32 examples each, shuffled in case of training set and not shuffled in case of validation set\n",
    "finetune_train_loader = DataLoader(\n",
    "    dataset=finetune_train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "finetune_val_loader = DataLoader(\n",
    "    dataset=finetune_val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "finetune_test_loader = DataLoader(\n",
    "    dataset=finetune_test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "print(\"=======================================================\")\n",
    "print(\"Number of training samples:\",len(finetune_train_dataset))\n",
    "print(\"Number of validation samples:\",len(finetune_val_dataset))\n",
    "print(\"Number of test samples:\",len(finetune_test_dataset))\n",
    "print(\"=======================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Print results function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(num_epochs, training_losses, validation_losses, \n",
    "                         training_accuracies, validation_accuracies, \n",
    "                         lr, output_folder='finetuning2'):\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot Losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, num_epochs + 1), training_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(range(1, num_epochs + 1), validation_losses, label='Validation Loss', marker='o', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot Accuracies\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, num_epochs + 1), training_accuracies, label='Training Accuracy', marker='o')\n",
    "    plt.plot(range(1, num_epochs + 1), validation_accuracies, label='Validation Accuracy', marker='o', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    image_name = f'{output_folder}/training_{num_epochs}_{lr}.png'\n",
    "    plt.savefig(image_name)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Plot saved as {image_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8imolEUuzAkt",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Default model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhAZhmvoqi44"
   },
   "source": [
    "### Train and Validation functions\n",
    "\n",
    "The optimizer chosen for the pretraining is Adam, with a low learning rate to perform a good fine tuning without overwriting weights coming from the pre-train.\\\n",
    "The fine-tuning model seems to be prone to overfitting, as different values of learning rate have been tested but yet the accuracy on the validation set after the 5th or 6th iteration started to grow. \\\n",
    "After trying different values for the learning rate (1e-4, 5e-5, and 1e-5), the best solution giving the best results over all epochs was: ADD CORRECT LEARNING RATE\n",
    "some other hyperparameters have been set in the optimizer to enhance the training phase:\n",
    " - beta values to control the momentum of the update of the learning rate and the sability of the updates, which is set to 0.8 insetead of the default 0.999\n",
    " - eps,  YET TO UNDERSTAND IF IT MAKES SENSE TO USE IT\n",
    " - weight decay, to penalize large weights and preserve the information coming from the pre-train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oK-G1UE2ttsq"
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device, transform=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0  # Added accuracy tracking during training\n",
    "\n",
    "    pbar = tqdm(dataloader, total=len(dataloader))\n",
    "    for batch in pbar:\n",
    "        images = batch[\"preprocessed_image\"].to(device, non_blocking=True)\n",
    "        texts = clip.tokenize(batch[\"sentence\"]).to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass with temperature scaling\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "        logits_per_image = logits_per_image / 0.07  # Temperature scaling\n",
    "        logits_per_text = logits_per_text / 0.07\n",
    "\n",
    "        # Compute loss\n",
    "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "        loss_image = loss_img(logits_per_image, ground_truth)\n",
    "        loss_text = loss_txt(logits_per_text, ground_truth)\n",
    "        loss = (loss_image + loss_text) / 2\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        # Add gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = torch.argmax(logits_per_image, dim=1)\n",
    "        accuracy = (predictions == ground_truth).float().mean()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy.item()\n",
    "\n",
    "        pbar.set_description(f'Loss: {loss.item():.4f} | Acc: {accuracy.item():.4f}')\n",
    "\n",
    "    return total_loss / len(dataloader), total_accuracy / len(dataloader)\n",
    "\n",
    "# Training wrapper with learning rate scheduling\n",
    "def train(model, train_loader, val_loader, num_epochs, device):\n",
    "\n",
    "    scheduler = torch.optim.CosineAnnealingWarmRestarts(\n",
    "        optimizer, \n",
    "        T_0=5,  # First restart cycle length\n",
    "        T_mult=2  # Multiply cycle length by 2 after each restart\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device)\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, val_loader, device)\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "            }, 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
    "        print(f'Current LR: {optimizer.param_groups[0][\"lr\"]:.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "am1sLw6urCfu"
   },
   "source": [
    "### Validation loop\n",
    "Training loop that generates the pretrained clip model on refCocog.\\\n",
    "Given the size of the dataset and the depth of the clip model, the number of epochs is set to 10.\\\n",
    "Reminding that the notebook was executed in a ml.g4dn.xlarge aws machine (the most powerful allowed as reported in the course's slides), the train for each epoch took about 21 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-9gTgwgq9ol",
    "outputId": "8d7d9af4-21b8-4b8b-dd08-13649b502a60"
   },
   "outputs": [],
   "source": [
    "# Learning rate and optimizer\n",
    "# lr=1e-6\n",
    "# # optimizer = Adam(clip_model.parameters(), lr=1e-3) # Overfit\n",
    "# # optimizer = Adam(clip_model.parameters(), lr=5e-5, betas=(0.9, 0.98), eps=1e-6, weight_decay=0.02) # Overfit\n",
    "# # optimizer = Adam(clip_model.parameters(), lr=5e-6, betas=(0.9, 0.98), weight_decay=0.02) # Overfit\n",
    "# # optimizer = Adam(clip_model.parameters(), lr=1e-6, betas=(0.9, 0.98), weight_decay=0.02) # Overfit\n",
    "# optimizer = Adam(clip_model.parameters(), lr=lr, betas=(0.9, 0.98), weight_decay=0.1) # Overfit\n",
    "\n",
    "\n",
    "# Learning rates form smallest to highest\n",
    "lrs = [5e-8, 1e-7, 5e-7, 1e-6, 5e-6]\n",
    "\n",
    "for lr in lrs:\n",
    "    # Scheduler for the learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=5,\n",
    "        T_mult=2\n",
    "    )\n",
    "\n",
    "    # Number of epochs for training\n",
    "    num_epochs = 10\n",
    "\n",
    "    # Store losses for plotting\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    validation_accuracies = []\n",
    "\n",
    "    # Values to get the best model\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "\n",
    "        # Train and Validate for one epoch\n",
    "        train_loss, train_accuracy = train_epoch(clip_model, finetune_train_loader, optimizer, device)\n",
    "        val_loss, val_accuracy = validate(clip_model, finetune_val_loader, device)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Store losses for plotting\n",
    "        training_losses.append(train_loss)\n",
    "        validation_losses.append(val_loss)\n",
    "        validation_accuracies.append(val_accuracy)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "            }, 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_accuracy:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}')\n",
    "        print(f'Current LR: {optimizer.param_groups[0][\"lr\"]:.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Plot Training and Validation Losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot Losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), training_losses, label='Training Loss', marker='o')\n",
    "plt.plot(range(1, num_epochs + 1), validation_losses, label='Validation Loss', marker='o', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Validation Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), validation_accuracies, label='Validation Accuracy', marker='o', color='tab:blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Display both plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the plots\n",
    "image_name = f'training_curves_finetuning1_{lr}.png'\n",
    "plt.savefig(image_name)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = validate(clip_model, finetune_test_loader, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom CLIP model\n",
    "After trying different parameters for the learning rate, from 5e-5 down to 5e-7, it is clear that to obtain a meaninful impact with the pretrain another approach has to be used.\\\n",
    "As the image above shows, very few improvements are done after the fourth epoch of finetuning, and the results on the validation test have not been very sarisfying as the validation accuracy increases but not consistenly and not significantly enough.\\\n",
    "For this reason I decided to add an attention layer at the end of CLIP. Hence the idea is to define a new model that does the image and text encoding, then apply a fully connected layer at the and, and again train the network, allowing the last layer to be fully trained by the clip images.\\\n",
    "This new model will be trainined leveragin the freezing layers technoque, where the last layers of the clip model will be freezed and no weight update will be applied and teh alst one and the first layers will be trained.\\\n",
    "The purpose of this is to let the network enhance its performances in the most general "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPGrounding(nn.Module):\n",
    "    def __init__(self, clip_model, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.clip_dim = self.clip_model.visual.output_dim\n",
    "\n",
    "        # Projection layers\n",
    "        # self.visual_projection = nn.Sequential(\n",
    "        #     nn.Linear(self.clip_dim, hidden_dim),\n",
    "        #     nn.LayerNorm(hidden_dim),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.1),\n",
    "        #     nn.Linear(hidden_dim, hidden_dim)\n",
    "        # )\n",
    "\n",
    "        # self.text_projection = nn.Sequential(\n",
    "        #     nn.Linear(self.clip_dim, hidden_dim),\n",
    "        #     nn.LayerNorm(hidden_dim),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.1),\n",
    "        #     nn.Linear(hidden_dim, hidden_dim)\n",
    "        # )\n",
    "\n",
    "        self.projection = nn.Linear(self.clip_dim, self.clip_dim)\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        # Encode images and text with CLIP\n",
    "\n",
    "        with torch.no_grad(): # Uncomment to don't train CLIP    \n",
    "            image_features = self.clip_model.encode_image(images)\n",
    "            text_tokens = clip.tokenize(texts).to(images.device)\n",
    "            text_features = self.clip_model.encode_text(text_tokens)\n",
    "\n",
    "        # # Ensure features are in the correct dtype\n",
    "        # image_features = image_features.to(self.visual_projection[0].weight.dtype)\n",
    "        # text_features = text_features.to(self.text_projection[0].weight.dtype)\n",
    "\n",
    "        # Project features through our layers\n",
    "        # image_features = self.visual_projection(image_features)\n",
    "        # text_features = self.text_projection(text_features)\n",
    "\n",
    "        # Simple projection\n",
    "        image_features = self.projection(image_features)\n",
    "        text_features = self.projection(text_features)\n",
    "\n",
    "        # Normalize features (add epsilon to avoid NaN)\n",
    "        image_features = F.normalize(image_features, dim=-1, eps=1e-6)\n",
    "        text_features = F.normalize(text_features, dim=-1, eps=1e-6)\n",
    "\n",
    "        return image_features, text_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Train and validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    # model.clip_model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "    for batch in pbar:\n",
    "        # print(\"batch: \",batch)\n",
    "        images = batch['preprocessed_image'].to(device)\n",
    "        texts = batch['sentence']\n",
    "\n",
    "        # print(\"images: \",images)\n",
    "        # print(\"texts: \",texts)\n",
    "\n",
    "        # Forward pass\n",
    "        image_features, text_features = model(images, texts)\n",
    "        # print(\"image_features: \",image_features)\n",
    "        # print(\"text_features: \",text_features)\n",
    "\n",
    "        # Calculate similarity matrix\n",
    "        similarity = image_features @ text_features.t()\n",
    "        # print(\"similarity: \",similarity)\n",
    "\n",
    "        # Labels for contrastive learning (diagonal is positive pairs)\n",
    "        labels = torch.arange(len(images)).to(device)\n",
    "\n",
    "        # Calculate loss (both image->text and text->image directions)\n",
    "        loss_i2t = F.cross_entropy(similarity, labels)\n",
    "        loss_t2i = F.cross_entropy(similarity.t(), labels)\n",
    "        # print(\"loss_i2t: \",loss_i2t)\n",
    "        # print(\"loss_t2i: \",loss_t2i)\n",
    "\n",
    "        loss = (loss_i2t + loss_t2i) / 2\n",
    "        # print(\"loss: \",loss)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = similarity.argmax(dim=1)\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_samples += len(images)\n",
    "\n",
    "        # Update progress bar\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100 * correct_predictions / total_samples:.2f}%'\n",
    "        })\n",
    "\n",
    "    return total_loss / len(train_loader), correct_predictions / total_samples\n",
    "\n",
    "def validate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='Validating'):\n",
    "            images = batch['preprocessed_image'].to(device)\n",
    "            texts = batch['sentence']\n",
    "\n",
    "            image_features, text_features = model(images, texts)\n",
    "            similarity = image_features @ text_features.t()\n",
    "\n",
    "            labels = torch.arange(len(images)).to(device)\n",
    "            loss_i2t = F.cross_entropy(similarity, labels)\n",
    "            loss_t2i = F.cross_entropy(similarity.t(), labels)\n",
    "            loss = (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "            predictions = similarity.argmax(dim=1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_samples += len(images)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(val_loader), correct_predictions / total_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Train and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rates form smallest to highest\n",
    "lrs = [5e-8, 1e-7, 5e-7, 1e-6, 5e-6]\n",
    "\n",
    "for lr in lrs:\n",
    "\n",
    "    # Create model\n",
    "    ft_clip_model = CLIPGrounding(clip_model).to(device)\n",
    "\n",
    "    # Print lr iteration information\n",
    "    print(f'Learning rate = {lr}:')\n",
    "\n",
    "    # number of epochs for each learnig rate\n",
    "    num_epochs = 10\n",
    "\n",
    "    # Best model's name\n",
    "    best_model_name = f'best_CLIPGrounding_{num_epochs}_{lr}.pth'\n",
    "\n",
    "    # Initialize optimizer with weight decay\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        ft_clip_model.parameters(),\n",
    "        lr = lr,\n",
    "        weight_decay=0.01,\n",
    "        eps=1e-8\n",
    "    )\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=num_epochs\n",
    "    )\n",
    "\n",
    "    # Lists to store metrics\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    training_accuracies = []\n",
    "    validation_accuracies = []\n",
    "\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # Train/validation loop\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # Train epoch\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            ft_clip_model, finetune_train_loader, optimizer, device, epoch\n",
    "        )\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(ft_clip_model, finetune_val_loader, device)\n",
    "\n",
    "        # Store metrics\n",
    "        training_losses.append(train_loss)\n",
    "        validation_losses.append(val_loss)\n",
    "        training_accuracies.append(train_acc)\n",
    "        validation_accuracies.append(val_acc)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print metrics\n",
    "        print(f'Epoch {epoch}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%')\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'ft_clip_model_state_dict': ft_clip_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, best_model_name)\n",
    "\n",
    "    # Print results\n",
    "    plot_training_curves(num_epochs,\n",
    "                         training_losses,\n",
    "                         validation_losses,\n",
    "                         training_accuracies,\n",
    "                         validation_accuracies,\n",
    "                         lr,\n",
    "                         output_folder='finetuning2')\n",
    "\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "https://learnopencv.com/yolo-loss-function-siou-focal-loss/ \\\n",
    "A class has been defined to implement SIoU loss function that will be used to train the models.\\\n",
    "Moreover, a function calculating MIoU and AP@50 and AP@75 have been implemented. These will be used just tas test metrics and to have a commond groud to be able to confront\\\n",
    "the base model and the custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(boxes1, boxes2):\n",
    "\n",
    "    # Calculate intersection coordinates\n",
    "    x1 = torch.max(boxes1[:, 0], boxes2[:, 0])\n",
    "    y1 = torch.max(boxes1[:, 1], boxes2[:, 1])\n",
    "    x2 = torch.min(boxes1[:, 2], boxes2[:, 2])\n",
    "    y2 = torch.min(boxes1[:, 3], boxes2[:, 3])\n",
    "\n",
    "    # Calculate intersection area\n",
    "    intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n",
    "\n",
    "    # Calculate union area\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "    union = area1 + area2 - intersection\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = intersection / (union + 1e-6)  # Add small epsilon to avoid division by zero\n",
    "    return iou\n",
    "\n",
    "\n",
    "def calculate_siou(pred_boxes, target_boxes):\n",
    "\n",
    "    '''\n",
    "        Part 1: Calculating classical IoU\n",
    "    '''\n",
    "    # Get box coordinates\n",
    "    pred_x1, pred_y1, pred_x2, pred_y2 = pred_boxes.chunk(4, dim=-1)\n",
    "    target_x1, target_y1, target_x2, target_y2 = target_boxes.chunk(4, dim=-1)\n",
    "\n",
    "    # Calculate box centers\n",
    "    pred_cx = (pred_x1 + pred_x2) / 2\n",
    "    pred_cy = (pred_y1 + pred_y2) / 2\n",
    "    target_cx = (target_x1 + target_x2) / 2\n",
    "    target_cy = (target_y1 + target_y2) / 2\n",
    "\n",
    "    # Box widths and heights\n",
    "    pred_w = pred_x2 - pred_x1\n",
    "    pred_h = pred_y2 - pred_y1\n",
    "    target_w = target_x2 - target_x1\n",
    "    target_h = target_y2 - target_y1\n",
    "\n",
    "    # Calculate intersection\n",
    "    x1 = torch.max(pred_x1, target_x1)\n",
    "    y1 = torch.max(pred_y1, target_y1)\n",
    "    x2 = torch.min(pred_x2, target_x2)\n",
    "    y2 = torch.min(pred_y2, target_y2)\n",
    "\n",
    "    # Calculate areas\n",
    "    intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n",
    "    pred_area = pred_w * pred_h\n",
    "    target_area = target_w * target_h\n",
    "    union = pred_area + target_area - intersection + 1e-7\n",
    "\n",
    "    # IoU\n",
    "    iou = intersection / union\n",
    "\n",
    "    '''\n",
    "        Part 2: Calculating angle, distance, and shape costs \n",
    "    '''\n",
    "    # Distance component\n",
    "    c_2 = torch.pow(target_cx - pred_cx, 2) + torch.pow(target_cy - pred_cy, 2)\n",
    "    c = torch.sqrt(c_2 + 1e-7)\n",
    "\n",
    "    # Diagonal length of the enclosing box\n",
    "    d = torch.sqrt(torch.pow(torch.max(pred_x2, target_x2) - torch.min(pred_x1, target_x1), 2) +\n",
    "                  torch.pow(torch.max(pred_y2, target_y2) - torch.min(pred_y1, target_y1), 2))\n",
    "\n",
    "    # Distance ratio\n",
    "    rho = c / (d + 1e-7)\n",
    "\n",
    "    # Angle component\n",
    "    pred_angle = torch.atan2(pred_cy - target_cy, pred_cx - target_cx)\n",
    "    target_angle = torch.atan2(target_h, target_w)\n",
    "    v = (4 / (math.pi ** 2)) * torch.pow(torch.atan2(target_w, target_h) - \n",
    "                                        torch.atan2(pred_w, pred_h), 2)\n",
    "\n",
    "    # Calculate alpha (trade-off parameter for angle cost)\n",
    "    alpha = v / (1 - iou + v + 1e-7)\n",
    "\n",
    "    # Angle cost\n",
    "    omega = v / (v + 1e-7)\n",
    "\n",
    "    '''\n",
    "        Part 2: Return final SIoU\n",
    "    '''\n",
    "    siou = 1 - iou + (rho + alpha * omega)\n",
    "\n",
    "    return siou.squeeze()\n",
    "\n",
    "class SIoULoss(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, pred_boxes, target_boxes, similarity_scores=None):\n",
    "\n",
    "        siou_loss = calculate_siou(pred_boxes, target_boxes)\n",
    "\n",
    "        # Add similarity score component if provided\n",
    "        if similarity_scores is not None:\n",
    "            # Convert similarity scores to a loss (1 - similarity)\n",
    "            similarity_loss = 1 - similarity_scores\n",
    "            # Combine losses (you can adjust the weighting)\n",
    "            combined_loss = siou_loss + 0.5 * similarity_loss\n",
    "        else:\n",
    "            combined_loss = siou_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return combined_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return combined_loss.sum()\n",
    "        else: \n",
    "            return combined_loss\n",
    "\n",
    "def calculate_test_metrics(pred_boxes, target_boxes, confidence_scores, iou_thresholds=[0.5, 0.75]):\n",
    "    # Calculate IoU for all predictions\n",
    "    ious = calculate_iou(pred_boxes, target_boxes)\n",
    "\n",
    "    # Calculate MIoU\n",
    "    miou = ious.mean().item()\n",
    "\n",
    "    # Calculate AP for each threshold\n",
    "    ap_dict = {}\n",
    "    for thresh in iou_thresholds:\n",
    "        # Sort by confidence scores\n",
    "        sorted_indices = torch.argsort(confidence_scores, descending=True)\n",
    "        sorted_ious = ious[sorted_indices]\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        positives = (sorted_ious >= thresh).float()\n",
    "        true_positives = torch.cumsum(positives, dim=0)\n",
    "        total_positives = torch.sum(positives)\n",
    "\n",
    "        if total_positives == 0:\n",
    "            ap_dict[f'ap_{int(thresh*100)}'] = 0.0\n",
    "            continue\n",
    "\n",
    "        precision = true_positives / torch.arange(1, len(true_positives) + 1, device=true_positives.device)\n",
    "        recall = true_positives / total_positives\n",
    "\n",
    "        # Calculate AP using 11-point interpolation\n",
    "        ap = 0.0\n",
    "        for t in torch.linspace(0, 1, 11):\n",
    "            if torch.sum(recall >= t) == 0:\n",
    "                continue\n",
    "            ap += torch.max(precision[recall >= t])\n",
    "        ap = ap / 11.0\n",
    "        ap_dict[f'ap_{int(thresh*100)}'] = ap.item()\n",
    "\n",
    "    return {'miou': miou, **ap_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZiwascG1hrj"
   },
   "source": [
    "## 1. Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoyviWj_83pG"
   },
   "source": [
    "### Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SECOND VERSION WITHOUT OPENING IMAGES\n",
    "class RefCOCOgDatasetModel(Dataset):\n",
    "    def __init__(self, transform=None, split='train', device='cuda', crop=False):\n",
    "        # Load images and transform\n",
    "        self.image_dir = os.path.join('refcocog', 'images')\n",
    "        self.transform = transform\n",
    "\n",
    "        # Define class properties for split and device\n",
    "        self.split = split\n",
    "        self.device = device\n",
    "        self.crop = crop\n",
    "\n",
    "        # Load data from ref(umd) and instances files\n",
    "        self.refs = self.load_refs()\n",
    "        self.instances = self.load_instances()\n",
    "\n",
    "        # Create efficient lookup dictionaries\n",
    "        self.image_id_to_filename = {img['id']: img['file_name'] \n",
    "                                   for img in self.instances['images']}\n",
    "        self.ann_id_to_bbox = {ann['id']: ann['bbox'] \n",
    "                              for ann in self.instances['annotations']}\n",
    "\n",
    "        # Prepare samples\n",
    "        self.samples = self._prepare_samples()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        # Load and process image\n",
    "        image_name = self.image_id_to_filename[sample['image_id']]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "\n",
    "        # Get and process bbox\n",
    "        bbox = self.ann_id_to_bbox[sample['ann_id']]\n",
    "\n",
    "        # Get and process bbox\n",
    "        box = self.ann_id_to_bbox[sample['ann_id']]\n",
    "        x1, y1, w, h = box\n",
    "        x2, y2 = x1 + w, y1 + h\n",
    "\n",
    "        # Optional: crop image to bounding boxes (for CLIP fine-tuning)\n",
    "        if self.crop:\n",
    "            # Ensure bbox coordinates are valid\n",
    "            x1 = max(0, int(x1))\n",
    "            y1 = max(0, int(y1))\n",
    "            x2 = min(image.size[0], int(x2))\n",
    "            y2 = min(image.size[1], int(y2))\n",
    "\n",
    "\n",
    "        # Sample to return\n",
    "        sample = {\n",
    "            'image_path':str(image_path),\n",
    "            'sentence': sample['sentence'],\n",
    "            'bbox': torch.Tensor([x1,y1,x2,y2])\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "    def load_refs(self):\n",
    "        annotation_file = os.path.join('refcocog', 'annotations', 'refs(umd).p')\n",
    "\n",
    "        with open(annotation_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return [item for item in data if item['split'] == self.split]\n",
    "\n",
    "    def load_instances(self):\n",
    "        instances_file = os.path.join('refcocog', 'annotations', 'instances.json')\n",
    "        with open(instances_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        samples = []\n",
    "        for ref in self.refs:\n",
    "            for sentence in ref['sentences']:\n",
    "                samples.append({\n",
    "                    'image_id': ref['image_id'],\n",
    "                    'ann_id': ref['ann_id'],\n",
    "                    'sentence': sentence['sent']\n",
    "                })\n",
    "        return samples\n",
    "\n",
    "# Define a function to open images\n",
    "def open_images(image_paths, preprocess):\n",
    "    # Ensure image_paths is a list\n",
    "    if isinstance(image_paths, str):\n",
    "        image_paths = [image_paths]\n",
    "\n",
    "    # Open and optionally preprocess all images\n",
    "    opened_images = [preprocess(Image.open(path)) if preprocess else Image.open(path) for path in image_paths]\n",
    "\n",
    "    # Return a single image if only one path was provided\n",
    "    return opened_images[0] if len(opened_images) == 1 else opened_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the goal of this model is to predict bounding boxes of an object in a picture from a textual description, new Dataset classes are created that do notapply any  crop or transformation to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test, and validation set split cropping images, without applying any transformation \n",
    "train_dataset = RefCOCOgDatasetModel(transform=preprocess,split='train')\n",
    "val_dataset = RefCOCOgDatasetModel(transform=preprocess,split='val')\n",
    "test_dataset = RefCOCOgDatasetModel(transform=preprocess,split='test')\n",
    "\n",
    "# DataLoaders batch size and other options. Computation is done with 4 workers to speed it up\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "pin_memory = True\n",
    "num_workers = 4\n",
    "persistent_workers = True\n",
    "\n",
    "# DataLoader, to create iterable batches with 32 examples each, shuffled in case of training set and not shuffled in case of test and validation sets\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "print(\"=======================================================\")\n",
    "print(\"Number of training samples:\",len(train_dataset))\n",
    "print(\"Number of validation samples:\",len(val_dataset))\n",
    "print(\"Number of test samples:\",len(test_dataset))\n",
    "print(\"=======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoyviWj_83pG",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8vsVZcZ1ovp"
   },
   "source": [
    "The base model is defined as a starting point to further study the task and become familiar with this visual grounding task.\\\n",
    "The approach is described in the project statement and is useful to get familiar with the visual grounding task. \\\n",
    "The idea is to feed the image inside YOLO to get the bounding boxes of all objects, apply to each the preprocessing and the find using clip the one that is the most close to the tokenizewd textual description. \\\n",
    "to find the object the closest to the description, the cosine similarity measure is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, yolo_model, clip_model, confidence_threshold=0.4, transform=None, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        # Initialize class' variables\n",
    "        self.device = device\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.transform = transform\n",
    "\n",
    "        # Initialize class' models\n",
    "        self.yolo_model = yolo_model\n",
    "        self.clip_model = clip_model\n",
    "        self.yolo_model.conf = confidence_threshold\n",
    "        self.yolo_model.to(device)\n",
    "        self.clip_model.to(device)\n",
    "\n",
    "    def forward(self, images_paths, descriptions):\n",
    "\n",
    "        # Lists to store results for the batch\n",
    "        batch_best_boxes = []\n",
    "        batch_best_scores = []\n",
    "        images_producing_no_bbox = {\n",
    "            \"paths\": [],\n",
    "            \"count\": 0\n",
    "        }\n",
    "        # Process each image in the batch\n",
    "        for idx in range(len(images_paths)):\n",
    "            image_path = images_paths[idx]\n",
    "            description = descriptions[idx]\n",
    "\n",
    "            # Get all objects with YOLO inference\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Get yolo objects\n",
    "                yolo_results = self.yolo_model(image_path)\n",
    "                crops = yolo_results.crop(save=False)\n",
    "\n",
    "                # Extract bounding boxes\n",
    "                boxes = [torch.tensor(crop['box'], device=self.device) for crop in crops]\n",
    "\n",
    "                # Prepare crops for CLIP\n",
    "                crops = process_crops_for_clip(crops, transform=self.transform, device=self.device)\n",
    "\n",
    "\n",
    "            # Convert list to batch tensor\n",
    "            if crops is not None and len(crops) > 0:\n",
    "                # Encode text description\n",
    "                with torch.no_grad():\n",
    "                    description_tokens = clip.tokenize(description).to(self.device)\n",
    "                    description_embedding = self.clip_model.encode_text(description_tokens).float()\n",
    "                    # Encode all object images at once\n",
    "                    object_embeddings = self.clip_model.encode_image(crops).float()\n",
    "\n",
    "                # Calculate similarities for all objects at once\n",
    "                similarities = torch.cosine_similarity(\n",
    "                    object_embeddings,\n",
    "                    description_embedding.repeat(len(object_embeddings), 1),\n",
    "                    dim=1\n",
    "                )\n",
    "\n",
    "                # Find best match\n",
    "                best_match_index = similarities.argmax()\n",
    "                best_match_box = boxes[best_match_index]  \n",
    "                best_match_score = similarities[best_match_index]\n",
    "            else:\n",
    "                # Handle case where transform failed for all objects\n",
    "                best_match_box = torch.zeros(4, device=self.device)\n",
    "                best_match_score = torch.tensor(0.0, device=self.device)\n",
    "                images_producing_no_bbox[\"paths\"].append(image_path)\n",
    "                images_producing_no_bbox[\"count\"] += 1\n",
    "\n",
    "            batch_best_boxes.append(best_match_box)\n",
    "            batch_best_scores.append(best_match_score)\n",
    "\n",
    "        # Stack results into tensors\n",
    "        pred_boxes = torch.stack(batch_best_boxes)\n",
    "        similarity_scores = torch.stack(batch_best_scores)\n",
    "\n",
    "        return pred_boxes, similarity_scores,images_producing_no_bbox\n",
    "\n",
    "\n",
    "def process_crops_for_clip(crops, transform=None, device='cuda'):\n",
    "    if transform is None:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                               (0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "\n",
    "    processed_crops = []\n",
    "    for crop in crops:\n",
    "        # Extract image from crop dictionary\n",
    "        if isinstance(crop, dict) and 'im' in crop:\n",
    "            crop_img = crop['im']  # Get the actual image from the dictionary\n",
    "        else:\n",
    "            crop_img = crop\n",
    "\n",
    "        # Convert numpy array to PIL Image if necessary\n",
    "        if isinstance(crop_img, np.ndarray):\n",
    "            crop_img = Image.fromarray(crop_img)\n",
    "\n",
    "        # Process the image with the provided transform\n",
    "        processed_crop = transform(crop_img).to(device)\n",
    "        processed_crops.append(processed_crop)\n",
    "\n",
    "    # Stack all processed crops into a batch\n",
    "    if processed_crops:\n",
    "        return torch.stack(processed_crops)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYKYEEyk7kxy",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Validation and test functions\n",
    "\n",
    "The loss function used to train the model is the IoU (Intersection over Union), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model,\n",
    "            val_loader,\n",
    "            criterion,\n",
    "            device):\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_iou = 0\n",
    "    total_samples = 0\n",
    "    total_no_bbox_images = 0\n",
    "\n",
    "    pbar = tqdm(val_loader, desc=\"Validating\")\n",
    "    for batch in pbar:\n",
    "        # Get batch data\n",
    "        images = batch['image_path']\n",
    "        sentences = batch[\"sentence\"]\n",
    "        target_boxes = batch['bbox'].to(device, non_blocking=True)\n",
    "\n",
    "        # Forward pass\n",
    "        pred_boxes, similarity_scores, no_bbox_images = model(images, sentences)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(pred_boxes, target_boxes, similarity_scores)\n",
    "\n",
    "        # Calculate metrics\n",
    "        batch_size = len(images)\n",
    "        total_samples += batch_size\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_no_bbox_images += no_bbox_images[\"count\"]\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{total_loss/total_samples:.4f}',\n",
    "            'total_no_bbox_images': f'{total_no_bbox_images}'\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / total_samples,\n",
    "        'total_no_bbox_images': total_no_bbox_images\n",
    "    }\n",
    "\n",
    "\n",
    "def test(model, val_loader, criterion, device):\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    total_no_bbox_images = 0\n",
    "\n",
    "    # Lists to store all predictions and targets for metric calculation\n",
    "    all_pred_boxes = []\n",
    "    all_target_boxes = []\n",
    "    all_confidence_scores = []\n",
    "\n",
    "    # For accuracy/precision calculation\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    pbar = tqdm(val_loader, desc=\"Validating\")\n",
    "    for batch in pbar:\n",
    "        # Get batch data\n",
    "        images = batch['image_path']\n",
    "        sentences = batch[\"sentence\"]\n",
    "        target_boxes = batch['bbox'].to(device, non_blocking=True)\n",
    "        # Forward pass\n",
    "        pred_boxes, similarity_scores, no_bbox_images = model(images, sentences)\n",
    "        # Calculate loss\n",
    "        loss = criterion(pred_boxes, target_boxes, similarity_scores)\n",
    "\n",
    "        # Calculate IoU between predictions and targets\n",
    "        ious = calculate_iou(pred_boxes, target_boxes)\n",
    "\n",
    "        # Consider a prediction correct if IoU > 0.5\n",
    "        correct_predictions = (ious > 0.5)\n",
    "\n",
    "        # Update metrics\n",
    "        true_positives += torch.sum(correct_predictions).item()\n",
    "        false_positives += torch.sum(~correct_predictions).item()\n",
    "\n",
    "        # Fix for false negatives calculation\n",
    "        total_targets = target_boxes.size(0)  # Number of target boxes\n",
    "        false_negatives += total_targets - torch.sum(correct_predictions).item()\n",
    "\n",
    "        # Store predictions and targets for metric calculation\n",
    "        all_pred_boxes.append(pred_boxes)\n",
    "        all_target_boxes.append(target_boxes)\n",
    "        all_confidence_scores.append(similarity_scores)\n",
    "\n",
    "        # Update totals\n",
    "        batch_size = len(images)\n",
    "        total_samples += batch_size\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_no_bbox_images += no_bbox_images[\"count\"]\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{total_loss/total_samples:.4f}',\n",
    "            'total_no_bbox_images': f'{total_no_bbox_images}'\n",
    "        })\n",
    "\n",
    "    # Calculate accuracy and precision\n",
    "    total_predictions = true_positives + false_positives + false_negatives\n",
    "    accuracy = true_positives / total_predictions if total_predictions > 0 else 0\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "\n",
    "    # Concatenate all predictions and targets\n",
    "    all_pred_boxes = torch.cat(all_pred_boxes)\n",
    "    all_target_boxes = torch.cat(all_target_boxes)\n",
    "    all_confidence_scores = torch.cat(all_confidence_scores)\n",
    "\n",
    "    # Calculate other metrics\n",
    "    metrics = calculate_test_metrics(\n",
    "        all_pred_boxes,\n",
    "        all_target_boxes,\n",
    "        all_confidence_scores,\n",
    "        iou_thresholds=[0.5, 0.75]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / total_samples,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'miou': metrics['miou'],\n",
    "        'ap_50': metrics['ap_50'],\n",
    "        'ap_75': metrics['ap_75'],\n",
    "        'total_no_bbox_images': total_no_bbox_images\n",
    "    }\n",
    "\n",
    "def print_metrics(name, metrics):\n",
    "    line_length = 50\n",
    "    print(\"\\n\" + \"=\" * line_length)\n",
    "    print(f\"{name} Metrics\".center(line_length))\n",
    "    print(\"=\" * line_length)\n",
    "    for key, value in metrics.items():\n",
    "        # Format percentage metrics\n",
    "        if key in ['miou', 'ap_50', 'ap_75', 'accuracy', 'precision']:\n",
    "            formatted_value = f\"{value*100:.1f}%\"\n",
    "        elif isinstance(value, float):\n",
    "            formatted_value = f\"{value:.4f}\"\n",
    "        else:\n",
    "            formatted_value = str(value)\n",
    "\n",
    "        # Pretty print each metric\n",
    "        key_display = key.replace('_', ' ').upper()\n",
    "        print(f\"{key_display:<20} : {formatted_value:>10}\")\n",
    "    print(\"-\" * line_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFEnidQR8pwb"
   },
   "source": [
    "Instantiating and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsx6uOd98pH8"
   },
   "outputs": [],
   "source": [
    "# Initialize model, criterion, optimizer\n",
    "base_model = BaseModel(yolo_model, clip_model)\n",
    "criterion = SIoULoss()\n",
    "\n",
    "# Test the model on the validation and test set \n",
    "val_metrics = validate(base_model, val_loader, criterion, device)\n",
    "test_metrics = test(base_model, test_loader, criterion, device)\n",
    "\n",
    "# Print both metrics\n",
    "print_metrics(\"Validation\", val_metrics)\n",
    "print_metrics(\"Test\", test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGMh259V3iho"
   },
   "source": [
    "## 2.  Custom Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test, and validation set split cropping images\n",
    "train_dataset = RefCOCOgDataset(transform=preprocess, split='train', crop=False)\n",
    "val_dataset = RefCOCOgDataset(transform=preprocess, split='val', crop=False)\n",
    "test_dataset = RefCOCOgDataset(transform=preprocess, split='test', crop=False)\n",
    "\n",
    "# DataLoaders batch size and other options. Computation is done with 4 workers to speed it up\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "pin_memory = True\n",
    "persistent_workers = True\n",
    "\n",
    "# DataLoader, to create iterable batches with 32 examples each, shuffled in case of training set and not shuffled in case of validation set\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "print(\"=======================================================\")\n",
    "print(\"Number of training samples:\",len(train_dataset))\n",
    "print(\"Number of validation samples:\",len(val_dataset))\n",
    "print(\"Number of test samples:\",len(test_dataset))\n",
    "print(\"=======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoyviWj_83pG"
   },
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8vsVZcZ1ovp"
   },
   "source": [
    "The idea of the custom model is rather simple, and somehow it resempbles what has been done for the pre-train.\\\n",
    "The modeltakes the fine-tuned CLIP anc concatenates layers to predict the bounding boxes. The idea is that the output of the encoded text and image are fed into the last layers to obtain the bounding boxes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel1(nn.Module):\n",
    "\n",
    "    def __init__(self, clip_grounding_model, hidden_dim=1024):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set the pretrained clip model\n",
    "        self.clip_grounding_model = clip_grounding_model\n",
    "\n",
    "        # concatenate a MLP for bounding boxes\n",
    "        self.bbox_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        # Extract global image and text features\n",
    "        image_features, text_features = self.clip_grounding_model(images, texts)\n",
    "\n",
    "        # Debug prints\n",
    "        # print(\"Image features shape:\", image_features.shape)\n",
    "        # print(\"Text features shape:\", text_features.shape)\n",
    "\n",
    "        # Normalize features\n",
    "        image_features = F.normalize(image_features, dim=-1, eps=1e-6)\n",
    "        text_features = F.normalize(text_features, dim=-1, eps=1e-6)\n",
    "\n",
    "        # Print combined shape\n",
    "        combined_features = torch.cat([image_features, text_features], dim=-1)\n",
    "        # print(\"Combined features shape:\", combined_features.shape)\n",
    "\n",
    "        # Predict bounding boxes\n",
    "        bounding_boxes = self.bbox_head(combined_features)\n",
    "        # print(\"Bounding boxes shape:\", bounding_boxes.shape)\n",
    "\n",
    "        return bounding_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYKYEEyk7kxy"
   },
   "source": [
    "### Train/Validation/Test functions\n",
    "\n",
    "The loss function used to train the model is the IoU (Intersection over Union), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    model.clip_grounding_model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    pbar = tqdm(dataloader, total=num_batches, desc='Training')\n",
    "\n",
    "    for batch in pbar:\n",
    "        try:\n",
    "            # Get preprocessed data from dataset\n",
    "            images = batch[\"preprocessed_image\"].to(device, non_blocking=True)\n",
    "            texts = batch[\"sentence\"]\n",
    "            target_boxes = batch[\"bbox\"].to(device, non_blocking=True)\n",
    "\n",
    "            # Training step\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            pred_boxes = model(images, texts)\n",
    "\n",
    "            loss = criterion(pred_boxes, target_boxes)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_description(f'Train Loss: {loss.item():.4f}')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, total=num_batches, desc='Validation')\n",
    "        for batch in pbar:\n",
    "            try:\n",
    "                # Get preprocessed data from dataset\n",
    "                images = batch[\"preprocessed_image\"].to(device, non_blocking=True)\n",
    "                texts = batch[\"sentence\"]\n",
    "                target_boxes = batch[\"bbox\"].to(device, non_blocking=True)\n",
    "\n",
    "                # Forward pass\n",
    "                pred_boxes = model(images, texts)\n",
    "                loss = criterion(pred_boxes, target_boxes)\n",
    "\n",
    "                # Update metrics\n",
    "                total_loss += loss.item()\n",
    "                all_preds.append(pred_boxes.cpu())\n",
    "                all_targets.append(target_boxes.cpu())\n",
    "\n",
    "                pbar.set_description(f'Val Loss: {loss.item():.4f}')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in validation batch: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    # Calculate metrics only if we have predictions\n",
    "    if all_preds and all_targets:\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "        metrics = calculate_test_metrics(\n",
    "            all_preds,\n",
    "            all_targets,\n",
    "            torch.ones(len(all_preds)),\n",
    "            iou_thresholds=[0.5, 0.75]\n",
    "        )\n",
    "        avg_loss = total_loss / num_batches\n",
    "        return avg_loss, metrics\n",
    "\n",
    "    return float('inf'), None\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    num_batches = len(test_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(test_loader, total=num_batches, desc='Testing')\n",
    "        for batch in pbar:\n",
    "            try:\n",
    "                # Get preprocessed data from dataset\n",
    "                images = batch[\"preprocessed_image\"].to(device, non_blocking=True)\n",
    "                texts = batch[\"sentence\"]\n",
    "                target_boxes = batch[\"bbox\"].to(device, non_blocking=True)\n",
    "\n",
    "                # Forward pass\n",
    "                pred_boxes = model(images, texts)\n",
    "                loss = criterion(pred_boxes, target_boxes)\n",
    "\n",
    "                # Update metrics\n",
    "                total_loss += loss.item()\n",
    "                all_preds.append(pred_boxes.cpu())\n",
    "                all_targets.append(target_boxes.cpu())\n",
    "\n",
    "                pbar.set_description(f'Test Loss: {loss.item():.4f}')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in test batch: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    # Calculate metrics only if we have predictions\n",
    "    if all_preds and all_targets:\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "        metrics = calculate_test_metrics(\n",
    "            all_preds,\n",
    "            all_targets,\n",
    "            torch.ones(len(all_preds)),\n",
    "            iou_thresholds=[0.5, 0.75]\n",
    "        )\n",
    "        avg_loss = total_loss / num_batches\n",
    "        return avg_loss, metrics\n",
    "\n",
    "    return float('inf'), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_grounding = CLIPGrounding(clip_model).to(device)\n",
    "custom_model = CustomModel1(clip_grounding).to(device)\n",
    "\n",
    "# Training parameters\n",
    "criterion = SIoULoss(reduction='mean').to(device)\n",
    "optimizer = torch.optim.AdamW(custom_model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "# Scheduler for the learning rate\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=5,\n",
    "    T_mult=2\n",
    ")\n",
    "\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 1\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "    # Train\n",
    "    train_loss = train_epoch(custom_model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "    # Validate\n",
    "    val_loss, val_metrics = validate(custom_model, val_loader, criterion, device)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step()\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': custom_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_metrics': val_metrics,\n",
    "        }, 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "    # Print metrics\n",
    "    print(f'Train Loss: {train_loss:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f}')\n",
    "    print('Validation Metrics:')\n",
    "    for k, v in val_metrics.items():\n",
    "        print(f'{k}: {v:.4f}')\n",
    "    print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on what to do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to consider when implementing the custom model are:\n",
    " - Data augmentation (increase the size of the dataset by applying transformations to the images)\n",
    " - Regularization techniques\n",
    " - Hyperparameters tuning (partially done in the fine-tuning part, as different values of learning rate have been tested)\n",
    "\n",
    "Evaluation is done on measures like:\n",
    " - localization accuracy\n",
    " - grounding accuracy\n",
    " - semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True, trust_repo=True) \n",
    "\n",
    "base_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),  # Convert PIL to tensor and normalize to [0,1]\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "        ])\n",
    "\n",
    "\n",
    "#Images returning an empty crop\n",
    "# refcocog/images/COCO_train2014_000000276874.jpg\n",
    "# refcocog/images/COCO_train2014_000000276874.jpg\n",
    "# refcocog/images/COCO_train2014_000000497807.jpg\n",
    "# refcocog/images/COCO_train2014_000000497807.jpg\n",
    "\n",
    "\n",
    "yolo_model.conf = 0.2\n",
    "\n",
    "im1 = 'refcocog/images/COCO_train2014_000000380440.jpg'\n",
    "im2 = 'refcocog/images/COCO_train2014_000000560180.jpg'\n",
    "\n",
    "\n",
    "# Load the images\n",
    "image1 = Image.open(im1)\n",
    "image2 = Image.open(im2)\n",
    "\n",
    "# image1 = base_transform(image1)\n",
    "# image2 = base_transform(image2)\n",
    "\n",
    "# image1 = image1.resize((640, 640))\n",
    "# image2 = image2.resize((640, 640))\n",
    "\n",
    "print(f\"Shape of Image 1: {image1.size}\")  # (width, height)\n",
    "print(f\"Shape of Image 2: {image2.size}\")\n",
    "\n",
    "# Show the images\n",
    "# image1.show(title=\"Image 1\")\n",
    "# image2.show(title=\"Image 2\")\n",
    "\n",
    "#Process first image\n",
    "results = yolo_model(im1)  # inference\n",
    "print(\"results image 1\")\n",
    "# print(\"result1\")\n",
    "# print(results) \n",
    "results.show()\n",
    "\n",
    "#Process second image\n",
    "results = yolo_model(im2)  # inference\n",
    "print(\"results image 2\")\n",
    "# print(\"result2\")\n",
    "# print(results)\n",
    "results.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "RVRByk6a0txX",
    "WGMh259V3iho",
    "uqu_koz-zW7W"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
