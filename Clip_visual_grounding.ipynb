{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcXWWy-L8pC6",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Environment setup \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXgkDdgMy00s"
   },
   "source": [
    "## Installing CLIP and YoloV5 and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UV4kP__zzi4v"
   },
   "source": [
    "It the first section of this file, the installation of the needed components is performed. These first bash lines install CLIP and YoloV5 respectively. These two Neural Network will represent the ground base of the project development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZafaZHs7O3Fn",
    "outputId": "0af4abe0-dabc-4d57-b72d-673f6d2ef4fd"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "# Download CLIP and YOLO\n",
    "pip install git+https://github.com/openai/CLIP.git\n",
    "pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt\n",
    "\n",
    "# Command to install some needed dependencies in the AWS machine\n",
    "sudo apt-get update && sudo apt-get install ffmpeg libsm6 libxext6 -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVRByk6a0txX"
   },
   "source": [
    "## List of imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24c7NKAZUpl3"
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "import pickle\n",
    "import json\n",
    "import tarfile\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# utility libraries imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# torch imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO26z6XD6K4b"
   },
   "source": [
    "## Setting the Clip model and Yolo model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dusz3KE86Jka",
    "outputId": "4b846792-9e80-4eeb-ba82-888648cc72e3"
   },
   "outputs": [],
   "source": [
    "# Chosing the device \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# choosing the clip model and the yolo versions, both pre-trained\n",
    "clip_model, preprocess = clip.load('RN50', device)\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True, trust_repo=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDi1EewJ6oNM"
   },
   "source": [
    "# Fine-tuning Clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1dkP5vnlaAu",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Creation of the train and validation splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GYXvuG3kW2c"
   },
   "source": [
    "The following code sections contain the needed structures to load the data from the refcoco dataset.\n",
    "The structures needed are:\n",
    "1. a Dataset Class to load the Data\n",
    "2. a Dataloader instantiation, to be used to split data into batches and that will be used to iterate throiugh for train, validation and test\n",
    "\n",
    "The purpose of the Refcocog is Referring Expression Grounding, whose goal is to identify an object given a referring example. This is corresponds with the objective of this project.\n",
    "\n",
    "The dataset is composed of 25799 images, each having an average of 3.7 referring expression. These expression are related to specific objects inside the image. The Ground truth is represented by the bounding boxes.\n",
    "\n",
    "The set of file composing the dataset are:\n",
    " - instances.json which contains all the information about the bunding boxes of each image\n",
    "   example of instance\n",
    " - ref(umd).p which is a serialized file with all the description related to a bounding box and the split it belongs to (train/validation/test)\n",
    " - the images directory with all the images\n",
    "\n",
    "This Dataset class, reads the instances.json and refs(umd).p files, creates an association image_id->image_name and annotation_id -> bounding_boxes to simplify the retrivial of the single element in the getitem() method.\\\n",
    "Moreover, a set of samples is created with all the datase entries, each seample is composed of: image id, annotation id, and the sentence. The oobjective of this structure, besides contaioning all samples for the len() method, is to simplify the implementation of the getitem method.\\\n",
    "The latter takes as input an idx (which is the element currently being processed by the iterator) and return the image cropped to the bounding boxes and the sentence related with that box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefCOCOgDataset(Dataset):\n",
    "    def __init__(self, transform=None, split='train', device='cuda', crop=False):\n",
    "        # Load images and transform\n",
    "        self.image_dir = os.path.join('refcocog', 'images')\n",
    "        self.transform = transform\n",
    "\n",
    "        # Define class properties for split and device\n",
    "        self.split = split\n",
    "        self.device = device\n",
    "        self.crop = crop\n",
    "\n",
    "        # Load data from ref(umd) and instances files\n",
    "        self.refs = self.load_refs()\n",
    "        self.instances = self.load_instances()\n",
    "\n",
    "        # Create efficient lookup dictionaries\n",
    "        self.image_id_to_filename = {img['id']: img['file_name'] \n",
    "                                   for img in self.instances['images']}\n",
    "        self.ann_id_to_bbox = {ann['id']: ann['bbox'] \n",
    "                              for ann in self.instances['annotations']}\n",
    "\n",
    "        # Prepare samples\n",
    "        self.samples = self._prepare_samples()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        # Load and process image\n",
    "        image_name = self.image_id_to_filename[sample['image_id']]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Get and process bbox\n",
    "        bbox = self.ann_id_to_bbox[sample['ann_id']]\n",
    "\n",
    "        # Get and process bbox\n",
    "        box = self.ann_id_to_bbox[sample['ann_id']]\n",
    "        x1, y1, w, h = box\n",
    "        x2, y2 = x1 + w, y1 + h\n",
    "\n",
    "        # Optional: crop image to bounding boxes (for CLIP fine-tuning)\n",
    "        if self.crop:\n",
    "            # Ensure bbox coordinates are valid\n",
    "            x1 = max(0, int(x1))\n",
    "            y1 = max(0, int(y1))\n",
    "            x2 = min(image.size[0], int(x2))\n",
    "            y2 = min(image.size[1], int(y2))\n",
    "\n",
    "            # Crop and transform\n",
    "            image = image.crop((x1, y1, x2, y2))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Sample to return\n",
    "        sample = {\n",
    "            'image_path':image_path,\n",
    "            'preprocessed_image': image,\n",
    "            'sentence': sample['sentence'],\n",
    "            'bbox': torch.Tensor([x1,y1,x2,y2])\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "    def load_refs(self):\n",
    "        annotation_file = os.path.join('refcocog', 'annotations', 'refs(umd).p')\n",
    "\n",
    "        with open(annotation_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return [item for item in data if item['split'] == self.split]\n",
    "\n",
    "    def load_instances(self):\n",
    "        instances_file = os.path.join('refcocog', 'annotations', 'instances.json')\n",
    "        with open(instances_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        samples = []\n",
    "        for ref in self.refs:\n",
    "            for sentence in ref['sentences']:\n",
    "                samples.append({\n",
    "                    'image_id': ref['image_id'],\n",
    "                    'ann_id': ref['ann_id'],\n",
    "                    'sentence': sentence['sent']\n",
    "                })\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different dasets class are instantiated, one for the train set, one for the validation test, and one for the test set. \n",
    "\n",
    "Each class is then loaded in a DataLoader wrapper. All these dataloader have been designed to work leveraging multithreading, with the goal of speeding up training and validation.\\\n",
    "It is important to point out that while the train set is shuffled, the validation and test set are not, since it wouyld be pointless to shuffle them. \\\n",
    "Moreover, data are split in batches whose size is 64. This parameter has also been chosen for speed reason, and 64 elements batches represent a good trade-of, since batches are nor too large or too somal, and the update of the weights happens after a reasonable amount of examples (given the dataset size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuQnqiR5ykkO"
   },
   "outputs": [],
   "source": [
    "# Train, test, and validation set split cropping images\n",
    "finetune_train_dataset = RefCOCOgDataset(transform=preprocess, split='train', crop=True)\n",
    "finetune_val_dataset   = RefCOCOgDataset(transform=preprocess, split='val', crop=True)\n",
    "\n",
    "# DataLoaders batch size and other options. Computation is done with 4 workers to speed it up\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "pin_memory = True\n",
    "persistent_workers = True\n",
    "\n",
    "# DataLoader, to create iterable batches with 32 examples each, shuffled in case of training set and not shuffled in case of validation set\n",
    "finetune_train_loader = DataLoader(\n",
    "    dataset=finetune_train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "finetune_val_loader = DataLoader(\n",
    "    dataset=finetune_val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "print(\"=======================================================\")\n",
    "print(\"Number of training samples:\",len(finetune_train_dataset))\n",
    "print(\"Number of validation samples:\",len(finetune_val_dataset))\n",
    "print(\"=======================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8imolEUuzAkt",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training and storing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhAZhmvoqi44"
   },
   "source": [
    "**Train** and **Validation** functions for each training epoch.\n",
    "\n",
    "The optimizer chosen for the pretraining is Adam, with a low learning rate to perform a good fine tuning without overwriting weights coming from the pre-train.\\\n",
    "The fine-tuning model seems to be prone to overfitting, as different values of learning rate have been tested but yet the accuracy on the validation set after the 5th or 6th iteration started to grow. \\\n",
    "After trying different values for the learning rate (1e-4, 5e-5, and 1e-5), the best solution giving the best results over all epochs was: ADD CORRECT LEARNING RATE\n",
    "some other hyperparameters have been set in the optimizer to enhance the training phase:\n",
    " - beta values to control the momentum of the update of the learning rate and the sability of the updates, which is set to 0.8 insetead of the default 0.999\n",
    " - eps,  YET TO UNDERSTAND IF IT MAKES SENSE TO USE IT\n",
    " - weight decay, to penalize large weights and preserve the information coming from the pre-train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oK-G1UE2ttsq"
   },
   "outputs": [],
   "source": [
    "# Learning rate and optimizer\n",
    "# learning_rate = 1e-3\n",
    "# optimizer = Adam(clip_model.parameters(), lr=learning_rate)\n",
    "# optimizer = Adam(clip_model.parameters(), lr=5e-5, betas=(0.9, 0.98), eps=1e-6, weight_decay=0.02)\n",
    "optimizer = Adam(clip_model.parameters(), lr=5e-6, betas=(0.9, 0.98), weight_decay=0.02)\n",
    "\n",
    "# Loss functions\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, dataloader, optimizer, device, transform=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, total=len(dataloader))\n",
    "\n",
    "    for batch in pbar:\n",
    "        images = batch[\"image\"].to(device, non_blocking=True)\n",
    "        texts = clip.tokenize(batch[\"sentence\"]).to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "        # Compute loss\n",
    "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "        loss_image = loss_img(logits_per_image, ground_truth)\n",
    "        loss_text = loss_txt(logits_per_text, ground_truth)\n",
    "        loss = (loss_image + loss_text) / 2\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_description(f'Loss: {loss.item():.4f}')\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, dataloader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    pbar = tqdm(dataloader, total=len(dataloader), desc='Validation')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            # Correctly extract images and texts from the batch\n",
    "            images = batch[\"image\"].to(device, non_blocking=True)\n",
    "            texts = clip.tokenize(batch[\"sentence\"]).to(device, non_blocking=True)\n",
    "            # Forward pass\n",
    "            logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "            # Calculate loss\n",
    "            ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "            loss_image = loss_img(logits_per_image, ground_truth)\n",
    "            loss_text = loss_txt(logits_per_text, ground_truth)\n",
    "            loss = (loss_image + loss_text) / 2\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(logits_per_image, dim=1)\n",
    "            accuracy = (predictions == ground_truth).float().mean()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += accuracy.item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_description(f'Val Loss: {loss.item():.4f} | Val Accuracy: {accuracy.item():.4f}')\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_accuracy = total_accuracy / len(dataloader)\n",
    "\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "am1sLw6urCfu"
   },
   "source": [
    "Training loop that generates the pretrained clip model on refCocog.\\\n",
    "Given the size of the dataset and the depth of the clip model, the number of epochs is set to 10.\\\n",
    "Reminding that the notebook was executed in a ml.g4dn.xlarge aws machine (the most powerful allowed as reported in the course's slides), the train for each epoch took about 21 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-9gTgwgq9ol",
    "outputId": "8d7d9af4-21b8-4b8b-dd08-13649b502a60"
   },
   "outputs": [],
   "source": [
    "    # Ensure the model is in float32 precision and transferred to the correct device\n",
    "    clip_model = clip_model.to(device).float()\n",
    "\n",
    "    # Number of epochs for training\n",
    "    num_epochs = 10\n",
    "\n",
    "    # Store losses for plotting\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    validation_accuracies = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, num_epochs + 1):  # Start epochs from 1 for readability\n",
    "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "\n",
    "        # Train and Validate for one epoch\n",
    "        train_loss = train_epoch(clip_model, finetune_train_loader, optimizer, device)\n",
    "        val_loss, val_accuracy = validate(clip_model, finetune_val_loader, device)\n",
    "\n",
    "        # Store losses for plotting\n",
    "        training_losses.append(train_loss)\n",
    "        validation_losses.append(val_loss)\n",
    "        validation_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save the final fine-tuned model\n",
    "    torch.save(clip_model.state_dict(), 'fine_tuned_clip_refcocog_final.pth')\n",
    "    print(\"\\nTraining complete. Model saved as 'fine_tuned_clip_refcocog_final.pth'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Plot Training and Validation Losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot Losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), training_losses, label='Training Loss', marker='o')\n",
    "plt.plot(range(1, num_epochs + 1), validation_losses, label='Validation Loss', marker='o', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Validation Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), validation_accuracies, label='Validation Accuracy', marker='o', color='tab:blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Display both plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_test_dataset = RefCOCOgDataset(transform=preprocess, split='test', crop=True)\n",
    "\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "pin_memory = True\n",
    "persistent_workers = True\n",
    "\n",
    "finetune_test_loader = DataLoader(\n",
    "    dataset=finetune_test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = validate(clip_model, finetune_test_loader, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoyviWj_83pG",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## New Train/validation/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SECOND VERSION WITHOUT OPENING IMAGES\n",
    "class RefCOCOgDatasetModel(Dataset):\n",
    "    def __init__(self, transform=None, split='train', device='cuda', crop=False):\n",
    "        # Load images and transform\n",
    "        self.image_dir = os.path.join('refcocog', 'images')\n",
    "        self.transform = transform\n",
    "\n",
    "        # Define class properties for split and device\n",
    "        self.split = split\n",
    "        self.device = device\n",
    "        self.crop = crop\n",
    "\n",
    "        # Load data from ref(umd) and instances files\n",
    "        self.refs = self.load_refs()\n",
    "        self.instances = self.load_instances()\n",
    "\n",
    "        # Create efficient lookup dictionaries\n",
    "        self.image_id_to_filename = {img['id']: img['file_name'] \n",
    "                                   for img in self.instances['images']}\n",
    "        self.ann_id_to_bbox = {ann['id']: ann['bbox'] \n",
    "                              for ann in self.instances['annotations']}\n",
    "\n",
    "        # Prepare samples\n",
    "        self.samples = self._prepare_samples()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        # Load and process image\n",
    "        image_name = self.image_id_to_filename[sample['image_id']]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "\n",
    "        # Get and process bbox\n",
    "        bbox = self.ann_id_to_bbox[sample['ann_id']]\n",
    "\n",
    "        # Get and process bbox\n",
    "        box = self.ann_id_to_bbox[sample['ann_id']]\n",
    "        x1, y1, w, h = box\n",
    "        x2, y2 = x1 + w, y1 + h\n",
    "\n",
    "        # Optional: crop image to bounding boxes (for CLIP fine-tuning)\n",
    "        if self.crop:\n",
    "            # Ensure bbox coordinates are valid\n",
    "            x1 = max(0, int(x1))\n",
    "            y1 = max(0, int(y1))\n",
    "            x2 = min(image.size[0], int(x2))\n",
    "            y2 = min(image.size[1], int(y2))\n",
    "\n",
    "\n",
    "        # Sample to return\n",
    "        sample = {\n",
    "            'image_path':str(image_path),\n",
    "            'sentence': sample['sentence'],\n",
    "            'bbox': torch.Tensor([x1,y1,x2,y2])\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "    def load_refs(self):\n",
    "        annotation_file = os.path.join('refcocog', 'annotations', 'refs(umd).p')\n",
    "\n",
    "        with open(annotation_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return [item for item in data if item['split'] == self.split]\n",
    "\n",
    "    def load_instances(self):\n",
    "        instances_file = os.path.join('refcocog', 'annotations', 'instances.json')\n",
    "        with open(instances_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        samples = []\n",
    "        for ref in self.refs:\n",
    "            for sentence in ref['sentences']:\n",
    "                samples.append({\n",
    "                    'image_id': ref['image_id'],\n",
    "                    'ann_id': ref['ann_id'],\n",
    "                    'sentence': sentence['sent']\n",
    "                })\n",
    "        return samples\n",
    "\n",
    "# Define a function to open images\n",
    "def open_images(image_paths, preprocess):\n",
    "    # Ensure image_paths is a list\n",
    "    if isinstance(image_paths, str):\n",
    "        image_paths = [image_paths]\n",
    "\n",
    "    # Open and optionally preprocess all images\n",
    "    opened_images = [preprocess(Image.open(path)) if preprocess else Image.open(path) for path in image_paths]\n",
    "\n",
    "    # Return a single image if only one path was provided\n",
    "    return opened_images[0] if len(opened_images) == 1 else opened_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the goal of this model is to predict bounding boxes of an object in a picture from a textual description, new Dataset classes are created that do notapply any  crop or transformation to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test, and validation set split cropping images, without applying any transformation \n",
    "train_dataset = RefCOCOgDatasetModel(transform=preprocess,split='train')\n",
    "val_dataset = RefCOCOgDatasetModel(transform=preprocess,split='val')\n",
    "test_dataset = RefCOCOgDatasetModel(transform=preprocess,split='test')\n",
    "\n",
    "# DataLoaders batch size and other options. Computation is done with 4 workers to speed it up\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "pin_memory = True\n",
    "num_workers = 4\n",
    "persistent_workers = True\n",
    "\n",
    "# DataLoader, to create iterable batches with 32 examples each, shuffled in case of training set and not shuffled in case of test and validation sets\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "print(\"=======================================================\")\n",
    "print(\"Number of training samples:\",len(train_dataset))\n",
    "print(\"Number of validation samples:\",len(val_dataset))\n",
    "print(\"Number of test samples:\",len(test_dataset))\n",
    "print(\"=======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "https://learnopencv.com/yolo-loss-function-siou-focal-loss/ \\\n",
    "A class has been defined to implement SIoU loss function that will be used to train the models.\\\n",
    "Moreover, a function calculating MIoU and AP@50 and AP@75 have been implemented. These will be used just tas test metrics and to have a commond groud to be able to confront\\\n",
    "the base model and the custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(boxes1, boxes2):\n",
    "\n",
    "    # Calculate intersection coordinates\n",
    "    x1 = torch.max(boxes1[:, 0], boxes2[:, 0])\n",
    "    y1 = torch.max(boxes1[:, 1], boxes2[:, 1])\n",
    "    x2 = torch.min(boxes1[:, 2], boxes2[:, 2])\n",
    "    y2 = torch.min(boxes1[:, 3], boxes2[:, 3])\n",
    "\n",
    "    # Calculate intersection area\n",
    "    intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n",
    "\n",
    "    # Calculate union area\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "    union = area1 + area2 - intersection\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = intersection / (union + 1e-6)  # Add small epsilon to avoid division by zero\n",
    "    return iou\n",
    "\n",
    "\n",
    "def calculate_siou(pred_boxes, target_boxes):\n",
    "\n",
    "    '''\n",
    "        Part 1: Calculating classical IoU\n",
    "    '''\n",
    "    # Get box coordinates\n",
    "    pred_x1, pred_y1, pred_x2, pred_y2 = pred_boxes.chunk(4, dim=-1)\n",
    "    target_x1, target_y1, target_x2, target_y2 = target_boxes.chunk(4, dim=-1)\n",
    "\n",
    "    # Calculate box centers\n",
    "    pred_cx = (pred_x1 + pred_x2) / 2\n",
    "    pred_cy = (pred_y1 + pred_y2) / 2\n",
    "    target_cx = (target_x1 + target_x2) / 2\n",
    "    target_cy = (target_y1 + target_y2) / 2\n",
    "\n",
    "    # Box widths and heights\n",
    "    pred_w = pred_x2 - pred_x1\n",
    "    pred_h = pred_y2 - pred_y1\n",
    "    target_w = target_x2 - target_x1\n",
    "    target_h = target_y2 - target_y1\n",
    "\n",
    "    # Calculate intersection\n",
    "    x1 = torch.max(pred_x1, target_x1)\n",
    "    y1 = torch.max(pred_y1, target_y1)\n",
    "    x2 = torch.min(pred_x2, target_x2)\n",
    "    y2 = torch.min(pred_y2, target_y2)\n",
    "\n",
    "    # Calculate areas\n",
    "    intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n",
    "    pred_area = pred_w * pred_h\n",
    "    target_area = target_w * target_h\n",
    "    union = pred_area + target_area - intersection + 1e-7\n",
    "\n",
    "    # IoU\n",
    "    iou = intersection / union\n",
    "\n",
    "    '''\n",
    "        Part 2: Calculating angle, distance, and shape costs \n",
    "    '''\n",
    "    # Distance component\n",
    "    c_2 = torch.pow(target_cx - pred_cx, 2) + torch.pow(target_cy - pred_cy, 2)\n",
    "    c = torch.sqrt(c_2 + 1e-7)\n",
    "\n",
    "    # Diagonal length of the enclosing box\n",
    "    d = torch.sqrt(torch.pow(torch.max(pred_x2, target_x2) - torch.min(pred_x1, target_x1), 2) +\n",
    "                  torch.pow(torch.max(pred_y2, target_y2) - torch.min(pred_y1, target_y1), 2))\n",
    "\n",
    "    # Distance ratio\n",
    "    rho = c / (d + 1e-7)\n",
    "\n",
    "    # Angle component\n",
    "    pred_angle = torch.atan2(pred_cy - target_cy, pred_cx - target_cx)\n",
    "    target_angle = torch.atan2(target_h, target_w)\n",
    "    v = (4 / (math.pi ** 2)) * torch.pow(torch.atan2(target_w, target_h) - \n",
    "                                        torch.atan2(pred_w, pred_h), 2)\n",
    "\n",
    "    # Calculate alpha (trade-off parameter for angle cost)\n",
    "    alpha = v / (1 - iou + v + 1e-7)\n",
    "\n",
    "    # Angle cost\n",
    "    omega = v / (v + 1e-7)\n",
    "\n",
    "    '''\n",
    "        Part 2: Return final SIoU\n",
    "    '''\n",
    "    siou = 1 - iou + (rho + alpha * omega)\n",
    "\n",
    "    return siou.squeeze()\n",
    "\n",
    "class SIoULoss(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, pred_boxes, target_boxes, similarity_scores=None):\n",
    "\n",
    "        siou_loss = calculate_siou(pred_boxes, target_boxes)\n",
    "\n",
    "        # Add similarity score component if provided\n",
    "        if similarity_scores is not None:\n",
    "            # Convert similarity scores to a loss (1 - similarity)\n",
    "            similarity_loss = 1 - similarity_scores\n",
    "            # Combine losses (you can adjust the weighting)\n",
    "            combined_loss = siou_loss + 0.5 * similarity_loss\n",
    "        else:\n",
    "            combined_loss = siou_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return combined_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return combined_loss.sum()\n",
    "        else: \n",
    "            return combined_loss\n",
    "\n",
    "def calculate_test_metrics(pred_boxes, target_boxes, confidence_scores, iou_thresholds=[0.5, 0.75]):\n",
    "    # Calculate IoU for all predictions\n",
    "    ious = calculate_iou(pred_boxes, target_boxes)\n",
    "\n",
    "    # Calculate MIoU\n",
    "    miou = ious.mean().item()\n",
    "\n",
    "    # Calculate AP for each threshold\n",
    "    ap_dict = {}\n",
    "    for thresh in iou_thresholds:\n",
    "        # Sort by confidence scores\n",
    "        sorted_indices = torch.argsort(confidence_scores, descending=True)\n",
    "        sorted_ious = ious[sorted_indices]\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        positives = (sorted_ious >= thresh).float()\n",
    "        true_positives = torch.cumsum(positives, dim=0)\n",
    "        total_positives = torch.sum(positives)\n",
    "\n",
    "        if total_positives == 0:\n",
    "            ap_dict[f'ap_{int(thresh*100)}'] = 0.0\n",
    "            continue\n",
    "\n",
    "        precision = true_positives / torch.arange(1, len(true_positives) + 1, device=true_positives.device)\n",
    "        recall = true_positives / total_positives\n",
    "\n",
    "        # Calculate AP using 11-point interpolation\n",
    "        ap = 0.0\n",
    "        for t in torch.linspace(0, 1, 11):\n",
    "            if torch.sum(recall >= t) == 0:\n",
    "                continue\n",
    "            ap += torch.max(precision[recall >= t])\n",
    "        ap = ap / 11.0\n",
    "        ap_dict[f'ap_{int(thresh*100)}'] = ap.item()\n",
    "\n",
    "    return {'miou': miou, **ap_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZiwascG1hrj"
   },
   "source": [
    "## 1. Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoyviWj_83pG"
   },
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8vsVZcZ1ovp"
   },
   "source": [
    "The base model is defined as a starting point to further study the task and become familiar with this visual grounding task.\\\n",
    "The approach is described in the project statement and is useful to get familiar with the visual grounding task. \\\n",
    "The idea is to feed the image inside YOLO to get the bounding boxes of all objects, apply to each the preprocessing and the find using clip the one that is the most close to the tokenizewd textual description. \\\n",
    "to find the object the closest to the description, the cosine similarity measure is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, yolo_model, clip_model, confidence_threshold=0.4, transform=None, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        # Initialize class' variables\n",
    "        self.device = device\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.transform = transform\n",
    "\n",
    "        # Initialize class' models\n",
    "        self.yolo_model = yolo_model\n",
    "        self.clip_model = clip_model\n",
    "        self.yolo_model.conf = confidence_threshold\n",
    "        self.yolo_model.to(device)\n",
    "        self.clip_model.to(device)\n",
    "\n",
    "        # # Move models to eval mode\n",
    "        # self.yolo_model.eval()\n",
    "        # self.clip_model.eval()\n",
    "\n",
    "    def forward(self, images_paths, descriptions):\n",
    "\n",
    "        # Lists to store results for the batch\n",
    "        batch_best_boxes = []\n",
    "        batch_best_scores = []\n",
    "        images_producing_no_bbox = {\n",
    "            \"paths\": [],\n",
    "            \"count\": 0\n",
    "        }\n",
    "        # Process each image in the batch\n",
    "        for idx in range(len(images_paths)):\n",
    "            image_path = images_paths[idx]\n",
    "            description = descriptions[idx]\n",
    "\n",
    "            # Get all objects with YOLO inference\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Get yolo objects\n",
    "                yolo_results = self.yolo_model(image_path)\n",
    "                crops = yolo_results.crop(save=False)\n",
    "\n",
    "                # Extract bounding boxes\n",
    "                boxes = [torch.tensor(crop['box'], device=self.device) for crop in crops]\n",
    "\n",
    "                # Prepare crops for CLIP\n",
    "                crops = process_crops_for_clip(crops, transform=self.transform, device=self.device)\n",
    "\n",
    "\n",
    "            # Convert list to batch tensor\n",
    "            if crops is not None and len(crops) > 0:\n",
    "                # Encode text description\n",
    "                with torch.no_grad():\n",
    "                    description_tokens = clip.tokenize(description).to(self.device)\n",
    "                    description_embedding = self.clip_model.encode_text(description_tokens).float()\n",
    "                    # Encode all object images at once\n",
    "                    object_embeddings = self.clip_model.encode_image(crops).float()\n",
    "\n",
    "                # Calculate similarities for all objects at once\n",
    "                similarities = torch.cosine_similarity(\n",
    "                    object_embeddings,\n",
    "                    description_embedding.repeat(len(object_embeddings), 1),\n",
    "                    dim=1\n",
    "                )\n",
    "\n",
    "                # Find best match\n",
    "                best_match_index = similarities.argmax()\n",
    "                best_match_box = boxes[best_match_index]  # Use the extracted boxes\n",
    "                best_match_score = similarities[best_match_index]\n",
    "            else:\n",
    "                # Handle case where transform failed for all objects\n",
    "                best_match_box = torch.zeros(4, device=self.device)\n",
    "                best_match_score = torch.tensor(0.0, device=self.device)\n",
    "                images_producing_no_bbox[\"paths\"].append(image_path)\n",
    "                images_producing_no_bbox[\"count\"] += 1\n",
    "\n",
    "            batch_best_boxes.append(best_match_box)\n",
    "            batch_best_scores.append(best_match_score)\n",
    "\n",
    "        # Stack results into tensors\n",
    "        pred_boxes = torch.stack(batch_best_boxes)\n",
    "        similarity_scores = torch.stack(batch_best_scores)\n",
    "\n",
    "        return pred_boxes, similarity_scores,images_producing_no_bbox\n",
    "\n",
    "\n",
    "def process_crops_for_clip(crops, transform=None, device='cuda'):\n",
    "    if transform is None:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                               (0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "\n",
    "    processed_crops = []\n",
    "    for crop in crops:\n",
    "        # Extract image from crop dictionary\n",
    "        if isinstance(crop, dict) and 'im' in crop:\n",
    "            crop_img = crop['im']  # Get the actual image from the dictionary\n",
    "        else:\n",
    "            crop_img = crop\n",
    "\n",
    "        # Convert numpy array to PIL Image if necessary\n",
    "        if isinstance(crop_img, np.ndarray):\n",
    "            crop_img = Image.fromarray(crop_img)\n",
    "\n",
    "        # Process the image with the provided transform\n",
    "        processed_crop = transform(crop_img).to(device)\n",
    "        processed_crops.append(processed_crop)\n",
    "\n",
    "    # Stack all processed crops into a batch\n",
    "    if processed_crops:\n",
    "        return torch.stack(processed_crops)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYKYEEyk7kxy"
   },
   "source": [
    "### Train/ValidationTest functions\n",
    "\n",
    "The loss function used to train the model is the IoU (Intersection over Union), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model,\n",
    "            val_loader,\n",
    "            criterion,\n",
    "            device):\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_iou = 0\n",
    "    total_samples = 0\n",
    "    total_no_bbox_images = 0\n",
    "\n",
    "    pbar = tqdm(val_loader, desc=\"Validating\")\n",
    "    for batch in pbar:\n",
    "        # Get batch data\n",
    "        images = batch['image_path']\n",
    "        sentences = batch[\"sentence\"]\n",
    "        target_boxes = batch['bbox'].to(device, non_blocking=True)\n",
    "\n",
    "        # Forward pass\n",
    "        pred_boxes, similarity_scores, no_bbox_images = model(images, sentences)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(pred_boxes, target_boxes, similarity_scores)\n",
    "\n",
    "        # Calculate metrics\n",
    "        batch_size = len(images)\n",
    "        total_samples += batch_size\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_no_bbox_images += no_bbox_images[\"count\"]\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{total_loss/total_samples:.4f}',\n",
    "            'total_no_bbox_images': f'{total_no_bbox_images}'\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / total_samples,\n",
    "        'total_no_bbox_images': total_no_bbox_images\n",
    "    }\n",
    "\n",
    "\n",
    "def test(model, val_loader, criterion, device):\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    total_no_bbox_images = 0\n",
    "\n",
    "    # Lists to store all predictions and targets for metric calculation\n",
    "    all_pred_boxes = []\n",
    "    all_target_boxes = []\n",
    "    all_confidence_scores = []\n",
    "\n",
    "    # For accuracy/precision calculation\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    pbar = tqdm(val_loader, desc=\"Validating\")\n",
    "    for batch in pbar:\n",
    "        # Get batch data\n",
    "        images = batch['image_path']\n",
    "        sentences = batch[\"sentence\"]\n",
    "        target_boxes = batch['bbox'].to(device, non_blocking=True)\n",
    "        # Forward pass\n",
    "        pred_boxes, similarity_scores, no_bbox_images = model(images, sentences)\n",
    "        # Calculate loss\n",
    "        loss = criterion(pred_boxes, target_boxes, similarity_scores)\n",
    "\n",
    "        # Calculate IoU between predictions and targets\n",
    "        ious = calculate_iou(pred_boxes, target_boxes)\n",
    "\n",
    "        # Consider a prediction correct if IoU > 0.5\n",
    "        correct_predictions = (ious > 0.5)\n",
    "\n",
    "        # Update metrics\n",
    "        true_positives += torch.sum(correct_predictions).item()\n",
    "        false_positives += torch.sum(~correct_predictions).item()\n",
    "        # Fix for false negatives calculation\n",
    "        total_targets = target_boxes.size(0)  # Number of target boxes\n",
    "        false_negatives += total_targets - torch.sum(correct_predictions).item()\n",
    "\n",
    "        # Store predictions and targets for metric calculation\n",
    "        all_pred_boxes.append(pred_boxes)\n",
    "        all_target_boxes.append(target_boxes)\n",
    "        all_confidence_scores.append(similarity_scores)\n",
    "\n",
    "        # Update totals\n",
    "        batch_size = len(images)\n",
    "        total_samples += batch_size\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_no_bbox_images += no_bbox_images[\"count\"]\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{total_loss/total_samples:.4f}',\n",
    "            'total_no_bbox_images': f'{total_no_bbox_images}'\n",
    "        })\n",
    "\n",
    "    # Calculate accuracy and precision\n",
    "    total_predictions = true_positives + false_positives + false_negatives\n",
    "    accuracy = true_positives / total_predictions if total_predictions > 0 else 0\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "\n",
    "    # Concatenate all predictions and targets\n",
    "    all_pred_boxes = torch.cat(all_pred_boxes)\n",
    "    all_target_boxes = torch.cat(all_target_boxes)\n",
    "    all_confidence_scores = torch.cat(all_confidence_scores)\n",
    "\n",
    "    # Calculate other metrics\n",
    "    metrics = calculate_test_metrics(\n",
    "        all_pred_boxes,\n",
    "        all_target_boxes,\n",
    "        all_confidence_scores,\n",
    "        iou_thresholds=[0.5, 0.75]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / total_samples,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'miou': metrics['miou'],\n",
    "        'ap_50': metrics['ap_50'],\n",
    "        'ap_75': metrics['ap_75'],\n",
    "        'total_no_bbox_images': total_no_bbox_images\n",
    "    }\n",
    "\n",
    "def print_metrics(name, metrics):\n",
    "    line_length = 50\n",
    "    print(\"\\n\" + \"=\" * line_length)\n",
    "    print(f\"{name} Metrics\".center(line_length))\n",
    "    print(\"=\" * line_length)\n",
    "    for key, value in metrics.items():\n",
    "        # Format percentage metrics\n",
    "        if key in ['miou', 'ap_50', 'ap_75', 'accuracy', 'precision']:\n",
    "            formatted_value = f\"{value*100:.1f}%\"\n",
    "        elif isinstance(value, float):\n",
    "            formatted_value = f\"{value:.4f}\"\n",
    "        else:\n",
    "            formatted_value = str(value)\n",
    "\n",
    "        # Pretty print each metric\n",
    "        key_display = key.replace('_', ' ').upper()\n",
    "        print(f\"{key_display:<20} : {formatted_value:>10}\")\n",
    "    print(\"-\" * line_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFEnidQR8pwb"
   },
   "source": [
    "Instantiating and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsx6uOd98pH8"
   },
   "outputs": [],
   "source": [
    "# Initialize model, criterion, optimizer\n",
    "base_model = BaseModel(yolo_model, clip_model)\n",
    "criterion = SIoULoss()\n",
    "\n",
    "# Test the model on the validation and test set \n",
    "val_metrics = validate(base_model, val_loader, criterion, device)\n",
    "test_metrics = test(base_model, test_loader, criterion, device)\n",
    "\n",
    "# Print both metrics\n",
    "print_metrics(\"Validation\", val_metrics)\n",
    "print_metrics(\"Test\", test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGMh259V3iho"
   },
   "source": [
    "## 2. XGBoost integration (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_tQeasK3tKG"
   },
   "source": [
    "After fine-tuning Clip and evaluating the base model, a custom model is defined for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Notes on what to do \n",
    "\n",
    "Things to consider when implementing the custom model are:\n",
    " - Data augmentation (increase the size of the dataset by applying transformations to the images)\n",
    " - Regularization techniques\n",
    " - Hyperparameters tuning (partially done in the fine-tuning part, as different values of learning rate have been tested)\n",
    "\n",
    "Evaluation is done on measures like:\n",
    " - localization accuracy\n",
    " - grounding accuracy\n",
    " - semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True, trust_repo=True) \n",
    "\n",
    "base_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),  # Convert PIL to tensor and normalize to [0,1]\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "        ])\n",
    "\n",
    "\n",
    "#Images returning an empty crop\n",
    "# refcocog/images/COCO_train2014_000000276874.jpg\n",
    "# refcocog/images/COCO_train2014_000000276874.jpg\n",
    "# refcocog/images/COCO_train2014_000000497807.jpg\n",
    "# refcocog/images/COCO_train2014_000000497807.jpg\n",
    "\n",
    "\n",
    "yolo_model.conf = 0.2\n",
    "\n",
    "im1 = 'refcocog/images/COCO_train2014_000000380440.jpg'\n",
    "im2 = 'refcocog/images/COCO_train2014_000000560180.jpg'\n",
    "\n",
    "\n",
    "# Load the images\n",
    "image1 = Image.open(im1)\n",
    "image2 = Image.open(im2)\n",
    "\n",
    "# image1 = base_transform(image1)\n",
    "# image2 = base_transform(image2)\n",
    "\n",
    "# image1 = image1.resize((640, 640))\n",
    "# image2 = image2.resize((640, 640))\n",
    "\n",
    "print(f\"Shape of Image 1: {image1.size}\")  # (width, height)\n",
    "print(f\"Shape of Image 2: {image2.size}\")\n",
    "\n",
    "# Show the images\n",
    "# image1.show(title=\"Image 1\")\n",
    "# image2.show(title=\"Image 2\")\n",
    "\n",
    "#Process first image\n",
    "results = yolo_model(im1)  # inference\n",
    "print(\"results image 1\")\n",
    "# print(\"result1\")\n",
    "# print(results) \n",
    "results.show()\n",
    "\n",
    "#Process second image\n",
    "results = yolo_model(im2)  # inference\n",
    "print(\"results image 2\")\n",
    "# print(\"result2\")\n",
    "# print(results)\n",
    "results.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "RVRByk6a0txX",
    "WGMh259V3iho",
    "uqu_koz-zW7W"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
