{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcXWWy-L8pC6"
   },
   "source": [
    "# Environment setup ============================================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXgkDdgMy00s"
   },
   "source": [
    "## Installing CLIP and YoloV5 and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UV4kP__zzi4v"
   },
   "source": [
    "It the first section of this file, the installation of the needed components is performed. These first bash lines install CLIP and YoloV5 respectively. These two Neural Network will represent the ground base of the project development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZafaZHs7O3Fn",
    "outputId": "0af4abe0-dabc-4d57-b72d-673f6d2ef4fd"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download CLIP and YOLO\n",
    "pip install git+https://github.com/openai/CLIP.git\n",
    "pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt\n",
    "\n",
    "# Command to install some needed dependencies in the AWS machine\n",
    "sudo apt-get update && sudo apt-get install ffmpeg libsm6 libxext6  -y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVRByk6a0txX"
   },
   "source": [
    "## List of imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24c7NKAZUpl3"
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "import pickle\n",
    "import json\n",
    "import tarfile\n",
    "import os\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# utility libraries imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# torch imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO26z6XD6K4b"
   },
   "source": [
    "## Setting the Clip model and Yolo model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dusz3KE86Jka",
    "outputId": "4b846792-9e80-4eeb-ba82-888648cc72e3"
   },
   "outputs": [],
   "source": [
    "# Chosing the device \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# choosing the clip model and the yolo versions\n",
    "clip_model, preprocess = clip.load('RN50', device)\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDi1EewJ6oNM"
   },
   "source": [
    "# Fine-tuning Clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1dkP5vnlaAu"
   },
   "source": [
    "## Creation of the train and validation splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GYXvuG3kW2c"
   },
   "source": [
    "Class definition for reading the RefCOCOg dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whC8D4Ytzktp"
   },
   "outputs": [],
   "source": [
    "class RefCOCOgDataset(Dataset):\n",
    "    def __init__(self, transform=None, split='train'):\n",
    "        # needed paths\n",
    "        self.image_dir = os.path.join( 'refcocog', 'images' )\n",
    "\n",
    "        # variables directly set\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "\n",
    "        # reading annotations and instances\n",
    "        self.refs = self.load_refs()\n",
    "        self.instances = self.load_instances()\n",
    "\n",
    "        # utils\n",
    "        self.image_id_to_filename = {img['id']: img['file_name'] for img in self.instances['images']}\n",
    "        self.ann_id_to_bbox = {ann['id']: ann['bbox'] for ann in self.instances['annotations']}\n",
    "\n",
    "        # define samples list\n",
    "        self.samples = []\n",
    "        for ref in self.refs:\n",
    "            for sentence in ref['sentences']:\n",
    "                self.samples.append({\n",
    "                    'image_id': ref['image_id'],\n",
    "                    'ann_id': ref['ann_id'],\n",
    "                    'sentence': sentence['sent']\n",
    "                })\n",
    "\n",
    "        # Preload all images into memory if you have enough RAM\n",
    "        # self.preloaded_images = {}\n",
    "        # for image_id, filename in self.image_id_to_filename.items():\n",
    "        #     image_path = os.path.join(self.image_dir, filename)\n",
    "        #     self.preloaded_images[image_id] = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # Precompute tokenization if possible\n",
    "        # self.tokenized_sentences = clip.tokenize([s['sentence'] for s in self.samples])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "         # Get image path and load image\n",
    "        image_name = self.image_id_to_filename.get(sample['image_id'])\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        # image = self.preloaded_images[sample['image_id']]\n",
    "\n",
    "        # Get bounding box\n",
    "        # box = [obj for obj in self.instances['annotations'] if obj['id'] == sample['ann_id']][0].get('bbox')\n",
    "        box = self.ann_id_to_bbox[sample['ann_id']]\n",
    "\n",
    "        # Crop image using the correct coordinate system:\n",
    "        x1 = max(0, int(box[0]))\n",
    "        y1 = max(0, int(box[1]))\n",
    "        x2 = min(image.size[0], int(box[0] + box[2]))\n",
    "        y2 = min(image.size[1], int(box[1] + box[3]))\n",
    "\n",
    "        # Crop the image to the bounding box region\n",
    "        cropped_image = image.crop((x1, y1, x2, y2))\n",
    "\n",
    "        # Apply transforms if specified\n",
    "        if self.transform:\n",
    "            cropped_image = self.transform(cropped_image)\n",
    "\n",
    "        # Keep the original bbox calculation as it was\n",
    "        bbox = torch.FloatTensor([\n",
    "            box[0],\n",
    "            box[1],\n",
    "            box[0] + box[2],\n",
    "            box[1] + box[3]\n",
    "        ])\n",
    "\n",
    "        return {\n",
    "            'image': cropped_image,\n",
    "            'bbox': bbox,\n",
    "            'sentence': sample['sentence']\n",
    "        }\n",
    "\n",
    "\n",
    "    def load_refs(self):\n",
    "        annotation_file = os.path.join('refcocog', 'annotations', 'refs(umd).p')\n",
    "        with open(annotation_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        annotations = [item for item in data if item['split'] == self.split]\n",
    "        return annotations\n",
    "\n",
    "    def load_instances(self):\n",
    "        instances_file = os.path.join('refcocog', 'annotations', 'instances.json')\n",
    "        return json.load(open(instances_file, 'r'))\n",
    "\n",
    "    def define_entries(self):\n",
    "        for img in self.instances['images']:\n",
    "            image_name = img[\"file_name\"]\n",
    "            images_annotations = [obj for obj in self.instances['annotations'] if obj['image_id'] == img[\"id\"]]\n",
    "            images_sentences = [obj for obj in self.refs if obj['image_id'] == img[\"id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuQnqiR5ykkO"
   },
   "outputs": [],
   "source": [
    "# Image transformation\n",
    "transform = None\n",
    "\n",
    "# #Train set and validation set split\n",
    "train_dataset = RefCOCOgDataset(transform=transform, split='train')\n",
    "val_dataset = RefCOCOgDataset(transform=transform, split='val')\n",
    "\n",
    "# DataLoaders batch size and other options. Computation is done with 4 workers to speed it up\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "num_workers = 1,\n",
    "pin_memory = True,\n",
    "persistent_workers = True\n",
    "\n",
    "# DataLoader, to create iterable batches with 32 examples each, shuffled in case of training set and not shuffled in case of validation set\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size,\n",
    "    shuffle,\n",
    "    # num_workers,\n",
    "    # pin_memory,\n",
    "    # persistent_workers\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size,\n",
    "    shuffle,\n",
    "    # num_workers,\n",
    "    # pin_memory,\n",
    "    # persistent_workers\n",
    ")\n",
    "\n",
    "# setting the transform property for the splits\n",
    "train_dataset.transform = preprocess\n",
    "val_dataset.transform = preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = next(iter(val_loader))\n",
    "batch2 = next(iter(val_loader))\n",
    "\n",
    "images1 = batch1['image'].to(device)\n",
    "texts1 = clip.tokenize(batch1['sentence']).to(device)\n",
    "\n",
    "logits_per_image1, logits_per_text1 = clip_model(images1, texts1)\n",
    "\n",
    "images2 = batch2['image'].to(device)\n",
    "texts2 = clip.tokenize(batch2['sentence']).to(device)\n",
    "\n",
    "logits_per_image2, logits_per_text2 = clip_model(images2, texts2)\n",
    "\n",
    "\n",
    "print(logits_per_image1)\n",
    "print(logits_per_text1)\n",
    "print(logits_per_image2)\n",
    "print(logits_per_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8imolEUuzAkt"
   },
   "source": [
    "## Training and storing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhAZhmvoqi44"
   },
   "source": [
    "**Train** and **Validation** functions for each training epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oK-G1UE2ttsq"
   },
   "outputs": [],
   "source": [
    "# Learning rate and optimizer\n",
    "learning_rate = 1e-3\n",
    "optimizer = Adam(clip_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loss function\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()  # Uncomment this line\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, total=len(dataloader))\n",
    "\n",
    "    for batch in pbar:\n",
    "        # Correctly extract images and texts from the batch\n",
    "        images = batch['image'].to(device)\n",
    "        texts = clip.tokenize(batch['sentence'] ).to(device)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # # Forward pass\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "        print(logits_per_image2)\n",
    "        print(logits_per_text2)\n",
    "\n",
    "        # # Compute loss (assuming you want to use the standard CLIP contrastive loss)\n",
    "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "        loss_image = loss_img(logits_per_image, ground_truth)\n",
    "        loss_text = loss_txt(logits_per_text, ground_truth)\n",
    "        loss = (loss_image + loss_text) / 2\n",
    "\n",
    "        print(\"loss: \", loss)\n",
    "\n",
    "        # # # Backward pass and optimization\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        # total_loss += loss.item()\n",
    "\n",
    "        # pbar.set_description(f'Loss: {loss.item():.4f}')\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            images = batch['image'].to(device)\n",
    "            texts = clip.tokenize(batch['sentence']).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "            # Calculate the loss\n",
    "            ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "            loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "am1sLw6urCfu"
   },
   "source": [
    "Training loop that generates the pretrained clip model on refCocog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-9gTgwgq9ol",
    "outputId": "8d7d9af4-21b8-4b8b-dd08-13649b502a60"
   },
   "outputs": [],
   "source": [
    "# Ensure the model is in train mode and using float32\n",
    "# clip_model = clip_model.float().train()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(clip_model, train_loader, optimizer, device)\n",
    "    val_loss = validate(clip_model, val_loader, device)\n",
    "\n",
    "    # print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    # print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    # print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    # print()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "torch.save(clip_model.state_dict(), 'fine_tuned_clip_refcocog.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZiwascG1hrj"
   },
   "source": [
    "# First Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoyviWj_83pG"
   },
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8vsVZcZ1ovp"
   },
   "source": [
    "The base model is defined as a starting point to further study the task and become familiar with this visual grounding task. The approach describe in the project statement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYKYEEyk7kxy"
   },
   "outputs": [],
   "source": [
    "class YoloClip(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, clip_model, yolo_model):\n",
    "        super().__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.yolo_model = yolo_model\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        # Obtain yolo detections for the original image\n",
    "        yolo_results = self.yolo_model(image)\n",
    "\n",
    "        # Get crops from the detections\n",
    "        yolo_crop = yolo_results.crop(save = False)\n",
    "\n",
    "        # Get the results in pandas form to get the number of crops for each image\n",
    "        yolo_panda = yolo_results.pandas().xyxy\n",
    "\n",
    "        crops_nums = [len(yolo_panda[i]) for i in range(len(image))]\n",
    "\n",
    "        # Preprocess the cropped images before passing it to CLIP\n",
    "        # crop_images = torch.stack([self.image_transform(crop['im'].copy()) for crop in yolo_crop]).to(device)\n",
    "        # crop_classes = [yolo_results.names.get(crop['cls'].item()) for crop in yolo_crop]\n",
    "        # crop_classes_tensor = names_to_coco_cats(crop_classes)\n",
    "        # crop_classes_tensor = crop_classes_tensor.to(device)\n",
    "\n",
    "        # Get the CLIP embedding for each of the cropped images\n",
    "        # image_features = self.clip_model.encode_image(crop_images)\n",
    "        # image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Get the CLIP embedding for each of the text tokens\n",
    "        # text = clip.tokenize(text).to(device)\n",
    "        # text_features = self.clip_model.encode_text(text)\n",
    "        # text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFEnidQR8pwb"
   },
   "source": [
    "Instantiating and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsx6uOd98pH8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGMh259V3iho"
   },
   "source": [
    "# Second Model: XGBoost integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_tQeasK3tKG"
   },
   "source": [
    "Describe the integration with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqu_koz-zW7W"
   },
   "source": [
    "# Bash utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OKAa_50TkWTv",
    "outputId": "7d3b3efc-975a-4d84-d3c1-4f6686fa3671"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "tar -xzvf refcocog.tar.gz"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "RVRByk6a0txX",
    "WGMh259V3iho",
    "uqu_koz-zW7W"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
