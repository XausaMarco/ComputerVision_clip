{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcXWWy-L8pC6"
   },
   "source": [
    "# Environment setup \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXgkDdgMy00s"
   },
   "source": [
    "## Installing CLIP and YoloV5 and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UV4kP__zzi4v"
   },
   "source": [
    "It the first section of this file, the installation of the needed components is performed. These first bash lines install CLIP and YoloV5 respectively. These two Neural Network will represent the ground base of the project development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZafaZHs7O3Fn",
    "outputId": "0af4abe0-dabc-4d57-b72d-673f6d2ef4fd"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "# Download CLIP and YOLO\n",
    "pip install git+https://github.com/openai/CLIP.git\n",
    "pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt\n",
    "\n",
    "# Command to install some needed dependencies in the AWS machine\n",
    "sudo apt-get update && sudo apt-get install ffmpeg libsm6 libxext6 -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVRByk6a0txX"
   },
   "source": [
    "## List of imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24c7NKAZUpl3"
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "import pickle\n",
    "import json\n",
    "import tarfile\n",
    "import os\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# utility libraries imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# torch imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO26z6XD6K4b"
   },
   "source": [
    "## Setting the Clip model and Yolo model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dusz3KE86Jka",
    "outputId": "4b846792-9e80-4eeb-ba82-888648cc72e3"
   },
   "outputs": [],
   "source": [
    "# Chosing the device \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# choosing the clip model and the yolo versions, both pre-trained\n",
    "clip_model, preprocess = clip.load('RN50', device)\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDi1EewJ6oNM"
   },
   "source": [
    "# Fine-tuning Clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1dkP5vnlaAu"
   },
   "source": [
    "## Creation of the train and validation splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GYXvuG3kW2c"
   },
   "source": [
    "The following code sections contain the needed structures to load the data from the refcoco dataset.\n",
    "The structures needed are:\n",
    "1. a Dataset Class to load the Data\n",
    "2. a Dataloader instantiation, to be used to split data into batches and that will be used to iterate throiugh for train, validation and test\n",
    "\n",
    "The purpose of the Refcocog is Referring Expression Grounding, whose goal is to identify an object given a referring example. This is corresponds with the objective of this project.\n",
    "\n",
    "The dataset is composed of 25799 images, each having an average of 3.7 referring expression. These expression are related to specific objects inside the image. The Ground truth is represented by the bounding boxes.\n",
    "\n",
    "The set of file composing the dataset are:\n",
    " - instances.json which contains all the information about the bunding boxes of each image\n",
    "   example of instance\n",
    " - ref(umd).p which is a serialized file with all the description related to a bounding box and the split it belongs to (train/validation/test)\n",
    " - the images directory with all the images\n",
    "\n",
    "This Dataset class, reads the instances.json and refs(umd).p files, creates an association image_id->image_name and annotation_id -> bounding_boxes to simplify the retrivial of the single element in the getitem() method.\\\n",
    "Moreover, a set of samples is created with all the datase entries, each seample is composed of: image id, annotation id, and the sentence. The oobjective of this structure, besides contaioning all samples for the len() method, is to simplify the implementation of the getitem method.\\\n",
    "The latter takes as input an idx (which is the element currently being processed by the iterator) and return the image cropped to the bounding boxes and the sentence related with that box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefCOCOgDataset(Dataset):\n",
    "    def __init__(self, transform=None, split='train', device='cuda', crop=False):\n",
    "        # Load images and transform\n",
    "        self.image_dir = os.path.join('refcocog', 'images')\n",
    "        self.transform = transform\n",
    "\n",
    "        # Define class properties for split and device\n",
    "        self.split = split\n",
    "        self.device = device\n",
    "        self.crop = crop\n",
    "\n",
    "        # Load data from ref(umd) and instances files\n",
    "        self.refs = self.load_refs()\n",
    "        self.instances = self.load_instances()\n",
    "\n",
    "        # Create efficient lookup dictionaries\n",
    "        self.image_id_to_filename = {img['id']: img['file_name'] \n",
    "                                   for img in self.instances['images']}\n",
    "        self.ann_id_to_bbox = {ann['id']: ann['bbox'] \n",
    "                              for ann in self.instances['annotations']}\n",
    "\n",
    "        # Prepare samples\n",
    "        self.samples = self._prepare_samples()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        # Load and process image\n",
    "        image_name = self.image_id_to_filename[sample['image_id']]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Get and process bbox\n",
    "        bbox = self.ann_id_to_bbox[sample['ann_id']]\n",
    "\n",
    "        # Get and process bbox\n",
    "        box = self.ann_id_to_bbox[sample['ann_id']]\n",
    "        x1, y1, w, h = box\n",
    "        x2, y2 = x1 + w, y1 + h\n",
    "\n",
    "        # Optional: crop image to bounding boxes (for CLIP fine-tuning)\n",
    "        if self.crop:\n",
    "            # Ensure bbox coordinates are valid\n",
    "            x1 = max(0, int(x1))\n",
    "            y1 = max(0, int(y1))\n",
    "            x2 = min(image.size[0], int(x2))\n",
    "            y2 = min(image.size[1], int(y2))\n",
    "\n",
    "            # Crop and transform\n",
    "            image = image.crop((x1, y1, x2, y2))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Sample to return\n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'sentence': sample['sentence'],\n",
    "            'bbox': bbox\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "    def load_refs(self):\n",
    "        annotation_file = os.path.join('refcocog', 'annotations', 'refs(umd).p')\n",
    "\n",
    "        with open(annotation_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return [item for item in data if item['split'] == self.split]\n",
    "\n",
    "    def load_instances(self):\n",
    "        instances_file = os.path.join('refcocog', 'annotations', 'instances.json')\n",
    "        with open(instances_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        samples = []\n",
    "        for ref in self.refs:\n",
    "            for sentence in ref['sentences']:\n",
    "                samples.append({\n",
    "                    'image_id': ref['image_id'],\n",
    "                    'ann_id': ref['ann_id'],\n",
    "                    'sentence': sentence['sent']\n",
    "                })\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different dasets class are instantiated, one for the train set, one for the validation test, and one for the test set. \n",
    "\n",
    "Each class is then loaded in a DataLoader wrapper. All these dataloader have been designed to work leveraging multithreading, with the goal of speeding up training and validation.\\\n",
    "It is important to point out that while the train set is shuffled, the validation and test set are not, since it wouyld be pointless to shuffle them. \\\n",
    "Moreover, data are split in batches whose size is 64. This parameter has also been chosen for speed reason, and 64 elements batches represent a good trade-of, since batches are nor too large or too somal, and the update of the weights happens after a reasonable amount of examples (given the dataset size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuQnqiR5ykkO"
   },
   "outputs": [],
   "source": [
    "# Train, test, and validation set split cropping images\n",
    "finetune_train_dataset = RefCOCOgDataset(transform=preprocess, split='train', crop=True)\n",
    "finetune_val_dataset   = RefCOCOgDataset(transform=preprocess, split='val', crop=True)\n",
    "\n",
    "# DataLoaders batch size and other options. Computation is done with 4 workers to speed it up\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "pin_memory = True\n",
    "persistent_workers = True\n",
    "\n",
    "# DataLoader, to create iterable batches with 32 examples each, shuffled in case of training set and not shuffled in case of validation set\n",
    "finetune_train_loader = DataLoader(\n",
    "    dataset=finetune_train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "finetune_val_loader = DataLoader(\n",
    "    dataset=finetune_val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "print(\"=======================================================\")\n",
    "print(\"Number of training samples:\",len(finetune_train_dataset))\n",
    "print(\"Number of validation samples:\",len(finetune_val_dataset))\n",
    "print(\"=======================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8imolEUuzAkt"
   },
   "source": [
    "## Training and storing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhAZhmvoqi44"
   },
   "source": [
    "**Train** and **Validation** functions for each training epoch.\n",
    "\n",
    "The optimizer chosen for the pretraining is Adam, with a low learning rate to perform a good fine tuning without overwriting weights coming from the pre-train.\\\n",
    "The fine-tuning model seems to be prone to overfitting, as different values of learning rate have been tested but yet the accuracy on the validation set after the 5th or 6th iteration started to grow. \\\n",
    "After trying different values for the learning rate (1e-4, 5e-5, and 1e-5), the best solution giving the best results over all epochs was: ADD CORRECT LEARNING RATE\n",
    "some other hyperparameters have been set in the optimizer to enhance the training phase:\n",
    " - beta values to control the momentum of the update of the learning rate and the sability of the updates, which is set to 0.8 insetead of the default 0.999\n",
    " - eps,  YET TO UNDERSTAND IF IT MAKES SENSE TO USE IT\n",
    " - weight decay, to penalize large weights and preserve the information coming from the pre-train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oK-G1UE2ttsq"
   },
   "outputs": [],
   "source": [
    "# Learning rate and optimizer\n",
    "# learning_rate = 1e-3\n",
    "# optimizer = Adam(clip_model.parameters(), lr=learning_rate)\n",
    "# optimizer = Adam(clip_model.parameters(), lr=5e-5, betas=(0.9, 0.98), eps=1e-6, weight_decay=0.02)\n",
    "optimizer = Adam(clip_model.parameters(), lr=5e-5, betas=(0.9, 0.98), weight_decay=0.02)\n",
    "\n",
    "# Loss functions\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, dataloader, optimizer, device, transform=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, total=len(dataloader))\n",
    "\n",
    "    for batch in pbar:\n",
    "        images = batch[\"image\"].to(device, non_blocking=True)\n",
    "        texts = clip.tokenize(batch[\"sentence\"]).to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "        # Compute loss\n",
    "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "        loss_image = loss_img(logits_per_image, ground_truth)\n",
    "        loss_text = loss_txt(logits_per_text, ground_truth)\n",
    "        loss = (loss_image + loss_text) / 2\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_description(f'Loss: {loss.item():.4f}')\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, dataloader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    pbar = tqdm(dataloader, total=len(dataloader), desc='Validation')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            # Correctly extract images and texts from the batch\n",
    "            images = batch[\"image\"].to(device, non_blocking=True)\n",
    "            texts = clip.tokenize(batch[\"sentence\"]).to(device, non_blocking=True)\n",
    "            # Forward pass\n",
    "            logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "            # Calculate loss\n",
    "            ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "            loss_image = loss_img(logits_per_image, ground_truth)\n",
    "            loss_text = loss_txt(logits_per_text, ground_truth)\n",
    "            loss = (loss_image + loss_text) / 2\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(logits_per_image, dim=1)\n",
    "            accuracy = (predictions == ground_truth).float().mean()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += accuracy.item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_description(f'Val Loss: {loss.item():.4f} | Val Accuracy: {accuracy.item():.4f}')\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_accuracy = total_accuracy / len(dataloader)\n",
    "\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "am1sLw6urCfu"
   },
   "source": [
    "Training loop that generates the pretrained clip model on refCocog.\\\n",
    "Given the size of the dataset and the depth of the clip model, the number of epochs is set to 10.\\\n",
    "Reminding that the notebook was executed in a ml.g4dn.xlarge aws machine (the most powerful allowed as reported in the course's slides), the train for each epoch took about 21 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-9gTgwgq9ol",
    "outputId": "8d7d9af4-21b8-4b8b-dd08-13649b502a60"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.8470:   3%|â–Ž         | 35/1258 [00:37<20:52,  1.02s/it]"
     ]
    }
   ],
   "source": [
    "    # Ensure the model is in float32 precision and transferred to the correct device\n",
    "    clip_model = clip_model.to(device).float()\n",
    "\n",
    "    # Number of epochs for training\n",
    "    num_epochs = 10\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, num_epochs + 1):  # Start epochs from 1 for readability\n",
    "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "\n",
    "        # Train and Validate for one epoch\n",
    "        train_loss = train_epoch(clip_model, finetune_train_loader, optimizer, device)\n",
    "        val_loss, val_accuracy = validate(clip_model, finetune_val_loader, device)\n",
    "\n",
    "        # Store losses for plotting\n",
    "        training_losses.append(train_loss)\n",
    "        validation_losses.append(val_loss)\n",
    "        validation_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save the final fine-tuned model\n",
    "    torch.save(clip_model.state_dict(), 'fine_tuned_clip_refcocog_final.pth')\n",
    "    print(\"\\nTraining complete. Model saved as 'fine_tuned_clip_refcocog_final.pth'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Validation Losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot Losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), training_losses, label='Training Loss', marker='o')\n",
    "plt.plot(range(1, num_epochs + 1), validation_losses, label='Validation Loss', marker='o', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Validation Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), validation_accuracies, label='Validation Accuracy', marker='o', color='tab:blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Display both plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoyviWj_83pG"
   },
   "source": [
    "## New Train/validation/test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the goal of this model is to predict bounding boxes of an object in a picture from a textual description, new Dataset classes are created that do notapply any  crop or transformation to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test, and validation set split cropping images, without applying any transformation \n",
    "train_dataset = RefCOCOgDataset(split='train')\n",
    "val_dataset = RefCOCOgDataset(split='val')\n",
    "test_dataset = RefCOCOgDataset(split='test')\n",
    "\n",
    "# DataLoaders batch size and other options. Computation is done with 4 workers to speed it up\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "pin_memory = True\n",
    "persistent_workers = True\n",
    "\n",
    "# DataLoader, to create iterable batches with 32 examples each, shuffled in case of training set and not shuffled in case of test and validation sets\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers\n",
    ")\n",
    "\n",
    "print(\"=======================================================\")\n",
    "print(\"Number of training samples:\",len(train_dataset))\n",
    "print(\"Number of validation samples:\",len(val_dataset))\n",
    "print(\"Number of test samples:\",len(test_dataset))\n",
    "print(\"=======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZiwascG1hrj"
   },
   "source": [
    "## 1. Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoyviWj_83pG"
   },
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8vsVZcZ1ovp"
   },
   "source": [
    "The base model is defined as a starting point to further study the task and become familiar with this visual grounding task.\\\n",
    "The approach is described in the project statement and is useful to get familiar with the visual grounding task. \\\n",
    "The idea is to feed the image inside YOLO to get the bounding boxes of all objects, apply to each the preprocessing and the find using clip the one that is the most close to the tokenizewd textual description. \\\n",
    "to find the object the closest to the description, the cosine similarity measure is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    # The constructor takes as input the YOLO and CLIP models, the confidence treshold for the object recognition and the transform preprocess to appply to each object\n",
    "    def __init__(self, yolo_model, clip_model, confidence_threshold = 0.5, transform = None, device = \"cuda\"):\n",
    "        super().__init__()\n",
    "        # Initialize class' variables\n",
    "        self.device = device\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.transform = transform\n",
    "\n",
    "        # Initialize class' models\n",
    "        self.yolo_model = yolo_model\n",
    "        self.clip_model = clip_model\n",
    "        self.yolo_model.to(device)\n",
    "        self.clip_model.to(device)\n",
    "\n",
    "    def forward(image, description):\n",
    "\n",
    "        image = image.to(device, non_blocking=True)\n",
    "        description = clip.tokenize(description).to(device, non_blocking=True)\n",
    "\n",
    "        # Get all objects with YOLO inference\n",
    "        yolo_results = self.yolo_model(image, conf=self.confidence_thresholdi)\n",
    "        # results.print()\n",
    "\n",
    "        # Extract bounding boxes and crop the image into multiple objects' images\n",
    "        yolo_results = yolo_results.xyxy[0]\n",
    "        objects = []\n",
    "        for result in yolo_results:\n",
    "            x1, y1, x2, y2, confidence, class_id = result.tolist()\n",
    "\n",
    "            # Crop image and apply transformation for CLIP\n",
    "            cropped_image = image.crop((x1, y1, x2, y2))\n",
    "            if self.transform:\n",
    "                cropped_image = self.transform(cropped_image)\n",
    "\n",
    "            # Append to the array of images\n",
    "            objects.append(cropped_image)\n",
    "\n",
    "        # Get description embedding\n",
    "        description_tokenized = clip_model.tokenize(description).to(self.device)\n",
    "        description_embedding = self.clip_model.encode_text(description_tokenized).float()\n",
    "\n",
    "        # Get objects images embeddings\n",
    "        objects_embeddings = []\n",
    "        for obj in objects:\n",
    "            obj = obj.to(self.device)\n",
    "            object_embedding = self.clip_model.encode_image(obj).float()\n",
    "            objects_embeddings.append(object_embedding)\n",
    "\n",
    "        # Calculating similarities between text and images\n",
    "        similarities = []\n",
    "        for object_embedding in objects_embeddings:\n",
    "            similarity = torch.cosine_similarity(object_embedding, description_embedding, dim=-1)\n",
    "            similarities.append(similarity.item())\n",
    "\n",
    "        # Find best similarity\n",
    "        best_match_index = torch.argmax(torch.tensor(similarities))\n",
    "        best_match_bbox = yolo_results[best_match_index][:4] \n",
    "        best_match_score = similarities[best_match_index]\n",
    "\n",
    "        # Return the best score\n",
    "        return best_match_bbox, best_match_score\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYKYEEyk7kxy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFEnidQR8pwb"
   },
   "source": [
    "Instantiating and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsx6uOd98pH8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGMh259V3iho"
   },
   "source": [
    "## 2. XGBoost integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_tQeasK3tKG"
   },
   "source": [
    "Describe the integration with XGBoost"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "RVRByk6a0txX",
    "WGMh259V3iho",
    "uqu_koz-zW7W"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
